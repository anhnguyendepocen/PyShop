{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyShop Session 1\n",
    "***\n",
    "## CPU and GPU Parallelization: How to Super Compute\n",
    "\n",
    "This session will introduce basic speed issues in the Python interpreter and the principles of parallelization.  We will work with a real problem, that of estimating an integral by monte carlo.  To begin, we'll see how to solve this problem in NumPy (hopefully a review at this point).  Then we'll see how to use just-in-time compiling (JIT) to increase speed with minimal effort.  Next, we'll discuss some of the basics of how a computer carries out computation, including a discussion of hard disks, ram, CPU, and cores.  We'll then look at some simple parallelization algorithms and how to break down a problem to be parallelized.  Finally, we'll cover how to use a graphics card to do GPU computation using PyCUDA, which can offer from 300 to 3000 times speed up over serial/native Python, and some publicly available resources.  By the end of this session you should understand the logic behind parallelization and have a general idea of what resources are available in Python.  You should also have an understanding of how to use PyCUDA to run code on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Python and Speed\n",
    "\n",
    "When you run code in Python, you benefit from the use of the interpreter.  This takes out the step of having to compile your code.  However, you suffer from the problem of dynamic typing.  This is what the interpreter must do in order to run your code dynamically.\n",
    "\n",
    "Dynamic typing refers to how the interpreter determines the type of data contained in an object.  Each object has associated to it a data type, and each time the interpreter encounters an object it must check to see what type it is.  This is contrary to C, where variables have hard coded data types as soon as they are defined.\n",
    "\n",
    "Beyond the issue of typing, Python has something called the \"Global Interpreter Lock (GIL)\".  This refers to the fact that Python only allows a single thread to run at any one point in time.  Because of this, sprouting threads could actually slow your code down as Python switches back and forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Working Example\n",
    "\n",
    "Say we would like to calculate the following expectation:\n",
    "\n",
    "$$\n",
    "\\phi = \\mathbb{E}[f(x)] = \\int_X f(x)g(x)dx \\\\\n",
    "X \\subset \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "If we would like to estimate this by a quadrature rule with, say, $100$ points in each dimension, then we need to make $100^n$ function evaluations, calculate the pdf $g(x)$ $100^n$ times, and generate $100^n$ quadrature weights in high dimensions.  This poses a massive computational problem and for dimensions higher than 10 is usually unfeasible (try it with scipy!).\n",
    "\n",
    "Alternatively, we can use monte carlo methods to leverage the law of large numbers to our advantage.  Given we know the law of $x$, we can generate random variables, take functional evaluations, and average to get an estimation of the expectation.  By the law of large numbers, this estimation will converge at a rate $\\frac{1}{\\sqrt{M}}$, where $M$ is the sample size.  That is, we can write\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f(x)] \\approx \\hat{\\phi} = \\frac{V}{M}\\sum_{i = 1}^M f(x_i)g(x_i) \n",
    "$$\n",
    "\n",
    "where $V=\\int_Xdx $ is the volume of $X$.  We can also get a variance estimate by\n",
    "\n",
    "$$\n",
    "\\sigma^2 \\approx \\hat{\\sigma^2} = \\frac{V^2}{M(M - 1)} \\sum_{i = 1}^M \\left(f(x_i)g(x_i) - \\frac{1}{V}\\hat{\\phi} \\right)^2\n",
    "$$\n",
    "\n",
    "We can code this in python fairly easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# NOTE: LLVM is not supported on ARM, so no numba\n",
    "from numba import jit\n",
    "# NOTE: No gpu on raspberry pi, so no pycuda\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "# NOTE: multiprocessing on raspberry pi?  you're joking. But wait! it's quad core!\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def monte_carlo_expectation_serial(f, g, M, X):\n",
    "    \"\"\"\n",
    "    A loop based monte carlo integration.\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    # Generate points uniformly from the sample space\n",
    "    N = X.shape[0]\n",
    "    points = np.random.rand(M, N)\n",
    "    \n",
    "    #Scale the points\n",
    "    points = points*(np.array([X[:, 1], ]*M)\n",
    "                     - np.array([X[:, 0], ]*M))\\\n",
    "        +  np.array([X[:, 0], ]*M)\n",
    "        \n",
    "    # Calculate the volume.  Assume a hyperrectangle for support.\n",
    "    V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "    # Calculate the sum as a loop\n",
    "    sum1 = 0.0\n",
    "    for i in range(0, M):\n",
    "        sum1 += f(points[i, :])*g(points[i, :], X)\n",
    "    sum1 /= M\n",
    "    \n",
    "    # Calculate the variance\n",
    "    var1 = 0.0\n",
    "    for i in range(0, M):\n",
    "        var1 += (f(points[i, :])*g(points[i, :], X) - sum1)**2\n",
    "\n",
    "    # Return the result\n",
    "    return V*sum1, V**2*var1/(M*(M - 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function takes in a support for the random variable (assumed to be a hyperrectangle), a moment function, a distribution function, and number of points to sample.  It returns an estimate for the expectation and its standard error.  This function is very flexible and we could define many different moments and istributions, but for now we will define the moment to be the euclidian norm and the distribution to be the n-dimensional uniform:\n",
    "\n",
    "$$\n",
    "f(x) = \\Vert x \\Vert^2 = \\sum_1^N x_i^2 \\\\\n",
    "g(x) = Uni(X) = \\prod_1^N\\frac{1}{\\overline{X_i} - \\underline{X_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we'll use numpy to calculate the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14310621492794479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 100\n",
    "N = 2\n",
    "points = np.random.rand(M, N)\n",
    "np.linalg.norm(points[0,:])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll define our functions $f$ and $g$ to pass to the monte carlo estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.linalg.norm(x)**2\n",
    "\n",
    "def g(x, X):\n",
    "    return np.prod(1/(X[:, 1] - X[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've said before, these functions are objects that we can pass to our estimator.  Finally, we can run it and see what we get.  We'll assume $x$ is standard uniform, so the true value should be $\\frac{N}{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40562423563716499, 0.0088462251727122083)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 10\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "monte_carlo_expectation_serial(f, g, M, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this gives us a value that is far from the truth (especially for high values of $N$, try it!) and with a fairly high standard error.  However, our sample size is just $10$.  Let's see what happens as $M$ increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.63712709429927616, 0.0019073392857243774)\n",
      "(0.67816661109642673, 0.00017961591501247626)\n",
      "(0.67330751666642097, 1.805657183897351e-05)\n"
     ]
    }
   ],
   "source": [
    "for M in [100, 1000, 10000]:\n",
    "    print(monte_carlo_expectation_serial(f, g, M, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also time our code to see how its doing.  Contrary to session 3, we'll use the `%timeit` magic.  This will run an optimum number of loops over our function (optimum in the sense that it won't take too long) and return a 'best of' run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.38 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100 loops, best of 3: 1.88 ms per loop\n",
      "100 loops, best of 3: 19 ms per loop\n",
      "1 loops, best of 3: 191 ms per loop\n"
     ]
    }
   ],
   "source": [
    "for M in [100, 1000, 10000]:\n",
    "    %timeit monte_carlo_expectation_serial(f, g, M, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably asking yourself why, after all this, are we using native Python again?!  It's just for exposition.  We can do the same thing in NumPy and compare.  Notice that we need to rewrite our functions to take array arguments and not vector arguments.  For `f` and `g` we add the `axis` argument to tell NumPy to operate along the '1'th axis and in the monte carlo estimation we replace the loop by a dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1],[2,2]])\n",
    "np.prod(np.ones((x.shape[0], x.shape[1]))/(X[:, 1] - X[:, 0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_vec(x):\n",
    "    return np.linalg.norm(x, axis = 1)**2\n",
    "\n",
    "def g_vec(x, X):\n",
    "    return np.prod(np.ones((x.shape[0], x.shape[1]))\n",
    "                   /(X[:, 1] - X[:, 0]), axis=1)\n",
    "\n",
    "def monte_carlo_expectation_numpy(f, g, M, X):\n",
    "    \"\"\"\n",
    "    A loop based monte carlo integration.\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    # Generate points uniformly from the sample space\n",
    "    N = X.shape[0]\n",
    "    points = np.random.rand(M, N)\n",
    "    \n",
    "    #Scale the points\n",
    "    points = points*(np.array([X[:, 1], ]*M)\n",
    "                     - np.array([X[:, 0], ]*M))\\\n",
    "        +  np.array([X[:, 0], ]*M)\n",
    "        \n",
    "    # Calculate the volume.  Assume a hyperrectangle for support.\n",
    "    V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "    # Calculate the sum as a dot product\n",
    "    sum1 = np.dot(f_vec(points), g_vec(points, X))/M\n",
    "    \n",
    "    # Calculate the variance\n",
    "    var1 = np.linalg.norm(f_vec(points)*g_vec(points, X) - sum1)**2\n",
    "\n",
    "    # Return the result\n",
    "    return V*sum1, V**2*var1/(M*(M - 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!  Now we can look and compare our results.  Keep in mind that the values are random, but should converge for high $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.58549310304394508, 0.0016324008286405893)\n",
      "(0.68501904409723469, 0.0018532056651179671)\n"
     ]
    }
   ],
   "source": [
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "print(monte_carlo_expectation_serial(f, g, M, X))\n",
    "print(monte_carlo_expectation_numpy(f_vec, g_vec, M, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!  Well, at least in the sense that we got about the same answer.  Now let's see how we do on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 100\n",
      "1000 loops, best of 3: 1.91 ms per loop\n",
      "10000 loops, best of 3: 118 µs per loop\n",
      "M = 1000\n",
      "100 loops, best of 3: 19 ms per loop\n",
      "1000 loops, best of 3: 712 µs per loop\n",
      "M = 10000\n",
      "10 loops, best of 3: 190 ms per loop\n",
      "100 loops, best of 3: 6.59 ms per loop\n"
     ]
    }
   ],
   "source": [
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "for M in [100, 1000, 10000]:\n",
    "    print(\"M = %s\" %M)\n",
    "    %timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "    %timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can really shout, \"Success!\"  Our NumPy function is 20 times faster than the native Python implimentation.  We could have also tried to use vectorize to move from native to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0c74be08b3e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvectorize_monte_carlo_serial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonte_carlo_expectation_serial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorize_monte_carlo_serial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/tmabbot/anaconda/envs/snakes/lib/python3.3/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1698\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1700\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tmabbot/anaconda/envs/snakes/lib/python3.3/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   1761\u001b[0m             \u001b[0m_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1763\u001b[1;33m             \u001b[0mufunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m             \u001b[1;31m# Convert args to object arrays first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tmabbot/anaconda/envs/snakes/lib/python3.3/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   1723\u001b[0m             \u001b[1;31m# arrays (the input values are not checked to ensure this)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1724\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_a\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1725\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1727\u001b[0m             \u001b[1;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f3bfaaadc2a5>\u001b[0m in \u001b[0;36mmonte_carlo_expectation_serial\u001b[1;34m(f, g, M, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Generate points uniformly from the sample space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "# This fails.  Our function is too complex for vectorize\n",
    "vectorize_monte_carlo_serial = np.vectorize(monte_carlo_expectation_serial)\n",
    "\n",
    "print(vectorize_monte_carlo_serial(f, g, M, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, it seems our function is too complex for vectorize.  We could further simplify it and vectorize sub-routines, but I'm not going to treat that here.\n",
    "\n",
    "So far we have simply repeated material we already know.  There are several other ways to speed up our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Based on Types\n",
    "\n",
    "As mentioned in the introduction, the dynamic typing done by the Python interpreter is one of the main drivers of sluggishness.  Given that, we can try to solve this problem by specifying types directly.  The two ways this is done is by using Cython and Numba.\n",
    "\n",
    "### Cython\n",
    "Throughout this course I've mentioned the fact that C is faster than Python.  Given this, some bright people created Cython in order to allow you to compile your Python code directly into C.  This requires a few simple modifications, changing variable definitions and some syntax, and you can achieve a large boost.  However, it requires a working compiler and can be rather tricky to debug.  Because of this, I won't be treating it here, but the <a href=\"http://docs.cython.org/src/tutorial/index.html\" target=\"_blank\">documntation</a> offers plenty of information to get you started if you're interested.\n",
    "\n",
    "### Numba\n",
    "The way around this problem of compilation and types is something called a \"just-in-time compiler\", or a JIT compiler.  JIT compilation works in a similar way to the Python interpreter, but with the major difference that it stores compiled code as it runs, returning to this code on later passes.  Therefore, when you call a function many times, it will be run as a pre-compiled C program on later passes.  This is a big black box and apparently the JIT compiler is hard for even computer scientists to understand, but it is sufficient to understand that it is the best of both worlds: no explicit compilation step and dynamic typing.\n",
    "\n",
    "Additionally, according to the Numba documentation, you could also reduce the memory footprint, but they do not say how or why...\n",
    "\n",
    "Using a just in time compiler requires simply importing it and adding the `@jit` decorator to your functions.  Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 318.95 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 1.46 ms per loop\n",
      "The slowest run took 1621.33 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 164 µs per loop\n",
      "0.001901388168334961\n",
      "0.0003018379211425781\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#If running on RasPi this wont work\n",
    "\n",
    "# Simply add the jit decorator before the function\n",
    "@jit\n",
    "def jitted_monte_carlo_serial(f, g, M, X):\n",
    "    \"\"\"\n",
    "    A loop based monte carlo integration.\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    # Generate points uniformly from the sample space\n",
    "    N = X.shape[0]\n",
    "    points = np.random.rand(M, N)\n",
    "    \n",
    "    #Scale the points\n",
    "    points = points*(np.array([X[:, 1], ]*M)\n",
    "                     - np.array([X[:, 0], ]*M)) +  np.array([X[:, 0], ]*M)\n",
    "        \n",
    "    # Calculate the volume.  Assume a hyperrectangle for support.\n",
    "    V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "    # Calculate the sum as a loop\n",
    "    sum1 = 0.0\n",
    "    for i in range(0, M):\n",
    "        sum1 += f(points[i, :])*g(points[i, :], X)\n",
    "    sum1 /= M\n",
    "    \n",
    "    # Calculate the variance\n",
    "    var1 = 0.0\n",
    "    for i in range(0, M):\n",
    "        var1 += (f(points[i, :]) - sum1)**2\n",
    "    \n",
    "    # Return the result\n",
    "    return V*sum1, V**2*var1/(M*(M - 1))\n",
    "\n",
    "@jit\n",
    "def jitted_monte_carlo_numpy(f, g, M, X):\n",
    "    \"\"\"\n",
    "    A loop based monte carlo integration.\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    # Generate points uniformly from the sample space\n",
    "    N = X.shape[0]\n",
    "    points = np.random.rand(M, N)\n",
    "    \n",
    "    #Scale the points\n",
    "    points = points*(np.array([X[:, 1], ]*M)\n",
    "                     - np.array([X[:, 0], ]*M)) +  np.array([X[:, 0], ]*M)\n",
    "        \n",
    "    # Calculate the volume.  Assume a hyperrectangle for support.\n",
    "    V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "    # Calculate the sum as a dot product\n",
    "    sum1 = np.dot(f_vec(points), g_vec(points, X))/M\n",
    "    \n",
    "    # Calculate the variance\n",
    "    var1 = np.linalg.norm(f_vec(points)*g_vec(points, X) - sum1)**2\n",
    "\n",
    "    # Return the result\n",
    "    return V*sum1, V**2*var1/(M*(M - 1))\n",
    "\n",
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "%timeit jitted_monte_carlo_serial(f, g, M, X)\n",
    "%timeit jitted_monte_carlo_numpy(f_vec, g_vec, M, X)\n",
    "\n",
    "start = time.time()\n",
    "jitted_monte_carlo_serial(f,g,M,X)\n",
    "print(time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "jitted_monte_carlo_numpy(f,g,M,X)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here the response of the `%timeit` magic.  The slowest run will be hundreds or even thousands of times slower than the fastest.  This is exactly what we expect, given that on subsequent runs it will use pre-compiled code.\n",
    "\n",
    "That being said, our function only runs once, so if we only want to estimate a single expectation, this doesn't help.  However, we could get some speed up from jitting the function calls within the estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 72.41 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1000 loops, best of 3: 1.57 ms per loop\n",
      "10000 loops, best of 3: 148 µs per loop\n"
     ]
    }
   ],
   "source": [
    "# However, to benefit you need to jit all functions\n",
    "@jit\n",
    "def jit_f(x):\n",
    "    return np.linalg.norm(x)**2\n",
    "@jit\n",
    "def jit_g(x, X):\n",
    "    return np.prod(1/(X[:, 1] - X[:, 0]))\n",
    "\n",
    "@jit\n",
    "def jit_f_vec(x):\n",
    "    return np.linalg.norm(x, axis = 1)**2\n",
    "\n",
    "@jit\n",
    "def jit_g_vec(x, X):\n",
    "    return np.prod(np.ones((x.shape[0], x.shape[1]))/(X[:, 1] - X[:, 0]), axis=1)\n",
    "\n",
    "\n",
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "%timeit jitted_monte_carlo_serial(jit_f, jit_g, M, X)\n",
    "%timeit jitted_monte_carlo_numpy(jit_f_vec, jit_g_vec, M, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you'll notice that this didn't quite work.  That is because we are already using numpy to do all of our calculations.  If we had non-standard moments that were not calculated using numpy functions, we would definitely get some boost.  Here's an example of some simple functions that speed up using `@jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 44.02 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100000 loops, best of 3: 2.34 µs per loop\n",
      "The slowest run took 82672.19 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1000000 loops, best of 3: 587 ns per loop\n"
     ]
    }
   ],
   "source": [
    "# Since we didn't get a speed up, let's see a simple example that does\n",
    "def test_func(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "@jit\n",
    "def jit_test_func(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "x = np.arange(1000)\n",
    "%timeit test_func(x)\n",
    "%timeit jit_test_func(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT compilation represents a truly plug-and-play modification to your code that could give you significant gains.  To get the most out of it, keep your code simple and loops short.  Use lots of functions calls and keep argument packing and unpacking to a minimum.\n",
    "\n",
    "Beyond type based speed ups, we can look at how to break down our problem into subproblems which are parallelizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "Before we discuss parallelization, it helps to discuss how a computer works (in broad terms).  The main parts of the computer are the hard disk, the ram (or memory), and the CPU.  The hard disk stores long term data.  The ram stores data for immediate use in calculation.  The CPU carries out calculation based on logical and arithmetical operations.\n",
    "\n",
    "### CPU Parallelization\n",
    "The CPU itself is made of several parts, which you can see in the diagram below.  Here is a list of the main ones:\n",
    "1. **Arithmetic Logic Unit (ALU)** - Executes arithmetical calculations and logic functions.  Also possible to have a floating point unit (FPU). \n",
    "2. **Control Unit** - Links incoming data, decodes it, and sends it to the execution unit.\n",
    "3. **Registers** - Temporary storage for data to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cpu.png\" width=\"500\" /img>\n",
    "Source: ccm.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all make up a core.  So for a multiprocessor there will be several sets of these components and a cache and bus to move things about.\n",
    "\n",
    "Taking advantage of multiple cores requires splitting up the work.  When we run code in Python, we run it **\"in serial\"**.  That is, we run a single chain of instructions on the CPU.  Our problem, however, may be **\"parallelizable\"**, meaning that we could simultaneously calculate portions of the solution and combine the sub-solutions at the end, running our code **\"in parallel\"**.  This is the idea behind using **\"parallelization\"**.\n",
    "\n",
    "#### Types of Parallelism\n",
    "There are several ways to speed up code using the idea of parallelization.  The first is called **\"bit-level parallelism\"**.  This is done directly on the CPU and is how you take advantage of Moore's law.  Since the introduction of transistor based computer hardware, technology has essentially followed Moore's law, which states that the number of transistors on a chip will double approximately every two years.  Roughly, that means that the speed of chips is increasing exponentially.  That being said, we are nearing the end of this exponential growth in the complexity of computer chip design.  Eventually (suprisingly soon), the transistors will be built on an atomic scale and the distance between them will become so small that chips cannot handle more density.  Eventually Moore's law will end.\n",
    "\n",
    "When someone discusses 32-bit versus 64-bit chip architecture, this is what they are talking about.  You as a programmer will never be able to interact with this type of parallelism.  Sorry.  This part was just for your culture.\n",
    "\n",
    "Secon, there is **\"instruction-level parallelism\"**.  This is the process of determining what functions or programs will be run in what order.  Again, this is beyond our ability (unless you are a computer scientist and didn't tell me), but it's nice to know.\n",
    "\n",
    "Finally, there is **\"task parallelism\"**.  This is what you can and will take advantage of in order to speed up your code.  This is the idea of breaking down your program into a series of very simplistic tasks and splitting them up into portions that can be run seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threaded v. Multiprocessing\n",
    "Given that you would like to leverage the task parallelism of your problem, you may have heard several terms referring to parallel computing: **threading** and **multiprocessing**.  Threading is when a single instance of a program sends out several sets of instructions and executes them seperately.  Multiprocessing refers to spawning several instances of a program and running the processes seperately.  This may seem like an unecessary differentiation now, but these two are very different.\n",
    "\n",
    "In particular, Python does not allow threading because of the **\"Global Interpreter Lock\"**, or GIL.  This allows the interpreter to run only a single rinstruction at a time.  In fact, running a multithreaded program could actually be slower than a serial one, as Python must switch between threads.  It is for this reason that we will be using multiprocessing for now.  This gets around the problem as each instance of the interpreter can run its own instructions.  That being said, in a moment we will do some GPU parallelization that uses threads, but this is a seperate issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Monte Carlo\n",
    "Monte carlo methods lends itself well to parallelization.  Why?  Because it is essentially just a sum, and this sum can be split up into partial sums.  So, to parallelize our estimation we simply split the original data into several parts, calculate the partial sums, then retrieve the data.  If that doesn't make sense, your homework is to write out the monte carlo problem as the sum of two partial sums.\n",
    "\n",
    "To do this, we'll make use of the `multiprocessing` package.  This contains functions for sprouting workers, executing the sub-routines, and gathering the data.  This is probably best explained in an example.  To make things more complicated, we'll define our first class of objects to estimate the expectation.\n",
    "\n",
    "Here's the code, heavily annotated to explain the logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cpu_parallel_monte_carlo:\n",
    "    \"\"\"\n",
    "    A class containing CPU based parallelization of the serial monte carlo\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    # Define initial characteristics\n",
    "    # NOTE:  When you define a variable of some class, this function is run.\n",
    "    # It initializes differeent characteristics of the object.\n",
    "    def __init__(self, f, g, M, X, workers):\n",
    "        # Check if the number of operations is divisible by the number of workers\n",
    "        # NOTE: This may not be an issue, but better safe than sorry\n",
    "        if M%workers is not 0:\n",
    "            raise ValueError('The vector of points must be evenly divisible'\n",
    "                             + ' by the number of workers.')\n",
    "        # Initialize the object attributes\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "        self.M = M\n",
    "        self.X = X\n",
    "        self.workers = workers\n",
    "        # mp.Queue() creates an object where you can store the output of the\n",
    "        # different processes as you go.  As you'll see, we can then recover\n",
    "        # this data from the queue\n",
    "        self.queue = mp.Queue()\n",
    "        self.N =  X.shape[0]\n",
    "        self.points = np.random.rand(M, N)*(np.array([X[:, 1], ]*M) \n",
    "                                           - np.array([X[:, 0], ]*M))\\\n",
    "            +  np.array([X[:, 0], ]*M)\n",
    "        self.V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "        # When the object is defined we go ahead and estimate.\n",
    "        # NOTE: we'll change this in a moment\n",
    "        self.mc_mean()\n",
    "        self.mc_var()\n",
    "\n",
    "\n",
    "    # Define functions that can be parallelized\n",
    "    # NOTE: This function simply uses the process number to take a slice of\n",
    "    # the points and then calculate the sum over these points.\n",
    "    def partial_sum(self, process):\n",
    "        \"\"\"\n",
    "        A function that will calculate the partial sum for the monte carlo\n",
    "        mean.\n",
    "        \n",
    "        inputs:\n",
    "            process   :    int; the process number\n",
    "\n",
    "        \"\"\"\n",
    "        # Unpack some info\n",
    "        # NOTE: This is the length of the slice of points.  We could have\n",
    "        # defined this as an attribute of the object itself, but oh well!\n",
    "        K = int(self.M/self.workers)\n",
    "        partial_points = self.points[process*K:(process + 1)*K, :]\n",
    "        \n",
    "        # Calculate the sum as a loop\n",
    "        sum1 = 0.0\n",
    "        for i in range(0, K):\n",
    "            sum1 += self.f(partial_points[i, :])*self.g(partial_points[i, :], self.X)\n",
    "            \n",
    "        # Return the calculated sum to the queue\n",
    "        # NOTE: This queue will store the output intil the processes\n",
    "        # have caught up to eachother\n",
    "        self.queue.put(sum1)\n",
    "    \n",
    "    # NOTE: This functionscalculates the partial sum for the standard\n",
    "    # error calculation.  The steps are identical, but the formula is different\n",
    "    def partial_var(self, process):\n",
    "        \"\"\"\n",
    "        A function that will calculate the partial sum for\n",
    "        the monte carlo variance.\n",
    "        \n",
    "        inputs:\n",
    "            process   :    int; the process number\n",
    "\n",
    "        \"\"\"\n",
    "        # Unpack some info\n",
    "        K = int(self.M/self.workers)\n",
    "        partial_points = self.points[process*K:(process + 1)*K, :]\n",
    "        \n",
    "        #Calculate the paritial sums as a loop\n",
    "        var1 = 0.0\n",
    "        sum1 = self.mean/self.V\n",
    "        for i in range(0, K):\n",
    "            var1 += (f(partial_points[i, :])*g(partial_points[i, :], X) - sum1)**2\n",
    "        self.queue.put(var1)\n",
    "    \n",
    "    # Calculate the estimation of the mean using multiprocessing\n",
    "    def mc_mean(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the mean by parallel montecarlo.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create a list of processes to run\n",
    "        # NOTE: mp.Process requires 2 arguments: target is the function to be run\n",
    "        # and kwargs are keyword arguments passed as a dictionary.  We create\n",
    "        # a list of processes to iterate over, one for each worker\n",
    "        processes = [mp.Process(target=self.partial_sum,\n",
    "                                kwargs=dict(process=i))\n",
    "                     for i in range(0, self.workers)]\n",
    "        \n",
    "        # Run the processes\n",
    "        for p in processes:\n",
    "            # This command tells the processes to run.\n",
    "            # NOTE: The processes do not need to wait for the others to finish in order to start.\n",
    "            p.start()\n",
    "        \n",
    "        # When the processes are done, exit\n",
    "        for p in processes:\n",
    "            # This command tells the spawning instance, the one running your program, to \n",
    "            # wait until process p has finished to continue.  So, on the first loop it will \n",
    "            # pause until the first process finishes, etc.\n",
    "            p.join()\n",
    "        \n",
    "        # Retrieve results from the queue and calculate the mean\n",
    "        # NOTE: queue.get() simply pops data out of the queue.  This is very similar\n",
    "        # to pop in lists.\n",
    "        partial_sums = [self.queue.get() for p in processes]\n",
    "        self.mean = sum(partial_sums)*self.V/self.M\n",
    "    \n",
    "    # Calculate the estimation of the variance using multiprocessing\n",
    "    # NOTE: The logic is the same as the preceding function\n",
    "    def mc_var(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the variance by parallel montecarlo.\n",
    "        \n",
    "        \"\"\"        \n",
    "        # Create a list of processes to run\n",
    "        processes = [mp.Process(target=self.partial_var,\n",
    "                                kwargs=dict(process=i))\n",
    "                     for i in range(0, self.workers)]\n",
    "        \n",
    "        # Run the processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        \n",
    "        # When the processes are done, exit\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # Retrieve results from the queue and calculate the mean\n",
    "        partial_sums = [self.queue.get() for p in processes]\n",
    "        self.var = sum(partial_sums)*self.V**2/(self.M*(self.M - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is quite long and takes a lot of thought on the first go.  This is the price you pay when you parallelize your problem.  The learning curve is steap and the logic can often be difficult.  However, the steps are always the same and you can often reuse code.  For instance, you never have to write a multiprocessor monte carlo again!  This one works for any moment and any distribution!  Well, I guess you need the distribution function, but come on.\n",
    "\n",
    "Ok, Let's see how the program performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.66089366292574248, 0.00018354183632094729)\n",
      "(0.66855723391600896, 0.00016819194662196142)\n",
      "(0.65451147105416585, 0.00017530210189413452)\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "M = 1000\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "test = cpu_parallel_monte_carlo(f, g, M, X, workers)\n",
    "\n",
    "print(monte_carlo_expectation_serial(f, g, M, X))\n",
    "print(monte_carlo_expectation_numpy(f_vec, g_vec, M, X))\n",
    "print((test.mean, test.var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  Our mean and variance estimates are the same in the serial, the numpy, and the multiprocessing application.  Now let's time it.\n",
    "\n",
    "First, let's see what happens with a single worker to see the overhead from sprouting workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 19.2 ms per loop\n",
      "1000 loops, best of 3: 718 µs per loop\n",
      "10 loops, best of 3: 51.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "M = 1000\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "# We can also time these three side by side\n",
    "%timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "%timeit cpu_parallel_monte_carlo(f, g, M, X, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy solution is still the clear winner, but notice that the muliprocessor solution is 3 to 10 times as slow as the plain serial implimentation.  This could be caused by the overhead of defining the class.  To abstract away from this, let's change our class to not calculate the mean and variance estimates initially, but to define them as methods.  **NOTE: When you do this you need to add a return statement to the funciton if you would like it to return anything, not simply calculate the value and add it as an attribute.  This depends on your implimentaiton, but I like it here for checking the output in a concise way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cpu_parallel_monte_carlo:\n",
    "    \"\"\"\n",
    "    A class containing CPU based parallelization of the serial monte carlo\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, f, g, M, X, workers):\n",
    "        # Check if the number of operations is divisible by the number of workers\n",
    "        if M%workers is not 0:\n",
    "            raise ValueError('The vector of points must be evenly divisible'\n",
    "                             + ' by the number of workers.')\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "        self.M = M\n",
    "        self.X = X\n",
    "        self.workers = workers\n",
    "        self.queue = mp.Queue()\n",
    "        self.N =  X.shape[0]\n",
    "        self.points = np.random.rand(M, N)*(np.array([X[:, 1], ]*M) \n",
    "                                           - np.array([X[:, 0], ]*M))\\\n",
    "            +  np.array([X[:, 0], ]*M)\n",
    "        self.V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    " \n",
    "    def partial_sum(self, process):\n",
    "        \"\"\"\n",
    "        A function that will calculate the partial sum for the monte carlo\n",
    "        mean.\n",
    "        \n",
    "        inputs:\n",
    "            process   :    int; the process number\n",
    "\n",
    "        \"\"\"\n",
    "        # Unpack some info\n",
    "        K = int(self.M/self.workers)\n",
    "        partial_points = self.points[process*K:(process + 1)*K, :]\n",
    "        \n",
    "        # Calculate the sum as a loop\n",
    "        sum1 = 0.0\n",
    "        for i in range(0, K):\n",
    "            sum1 += self.f(partial_points[i, :])*self.g(partial_points[i, :], self.X)\n",
    "            \n",
    "        # Return the calculated sum to the queue\n",
    "        self.queue.put(sum1)\n",
    "    \n",
    "    def partial_var(self, process):\n",
    "        \"\"\"\n",
    "        A function that will calculate the partial sum for\n",
    "        the monte carlo variance.\n",
    "        \n",
    "        inputs:\n",
    "            process   :    int; the process number\n",
    "\n",
    "        \"\"\"\n",
    "        # Unpack some info\n",
    "        K = int(self.M/self.workers)\n",
    "        partial_points = self.points[process*K:(process + 1)*K, :]\n",
    "        \n",
    "        #Calculate the paritial sums as a loop\n",
    "        var1 = 0.0\n",
    "        sum1 = self.mean/self.V\n",
    "        for i in range(0, K):\n",
    "            var1 += (f(partial_points[i, :])*g(partial_points[i, :], X) - sum1)**2\n",
    "        self.queue.put(var1)\n",
    "    \n",
    "    # Calculate the estimation of the mean using multiprocessing\n",
    "    def mc_mean(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the mean by parallel montecarlo.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create a list of processes to run\n",
    "        processes = [mp.Process(target=self.partial_sum,\n",
    "                                kwargs=dict(process=i))\n",
    "                     for i in range(0, self.workers)]\n",
    "        \n",
    "        # Run the processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        \n",
    "        # When the processes are done, exit\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # Retrieve results from the queue and calculate the mean\n",
    "        partial_sums = [self.queue.get() for p in processes]\n",
    "        self.mean = sum(partial_sums)*self.V/self.M\n",
    "        return self.mean\n",
    "    \n",
    "    # Calculate the estimation of the variance using multiprocessing\n",
    "    def mc_var(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the variance by parallel montecarlo.\n",
    "        \n",
    "        \"\"\"        \n",
    "        # Create a list of processes to run\n",
    "        processes = [mp.Process(target=self.partial_var,\n",
    "                                kwargs=dict(process=i))\n",
    "                     for i in range(0, self.workers)]\n",
    "        \n",
    "        # Run the processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        \n",
    "        # When the processes are done, exit\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # Retrieve results from the queue and calculate the mean\n",
    "        partial_sums = [self.queue.get() for p in processes]\n",
    "        self.var = sum(partial_sums)*self.V**2/(self.M*(self.M - 1))\n",
    "        return self.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 1.93 ms per loop\n",
      "10000 loops, best of 3: 120 µs per loop\n",
      "10 loops, best of 3: 42.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "test = cpu_parallel_monte_carlo(f, g, M, X, workers)\n",
    "\n",
    "# We can also time these three side by side\n",
    "%timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "%timeit (test.mc_mean(), test.mc_var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not solve the problem and is a clear example of how much overhead is faced in running several Python instances simultaneously.  \n",
    "\n",
    "In terms of speed, let's see how the serial version compares to the parallel one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 100\n",
      "The slowest run took 4.76 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100 loops, best of 3: 1.92 ms per loop\n",
      "10 loops, best of 3: 42.1 ms per loop\n",
      "M = 1000\n",
      "100 loops, best of 3: 19.2 ms per loop\n",
      "10 loops, best of 3: 49.5 ms per loop\n",
      "M = 10000\n",
      "10 loops, best of 3: 192 ms per loop\n",
      "10 loops, best of 3: 136 ms per loop\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "for M in [100, 1000, 10000]:\n",
    "    print(\"M = %s\" %M)\n",
    "    test = cpu_parallel_monte_carlo(f, g, M, X, workers)\n",
    "    %timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "    %timeit (test.mc_mean(), test.mc_var())\n",
    "    del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got speed!  Notice, though that we needed $M$ to be quite large to benefit.  This is because we only have four workers and large overhead for getting them started.  If you're computer is faster than mine, run the above up to 100000, or even 1000000 if you have a lot of memory!\n",
    "\n",
    "Ok, so we've done the above using loops, but what if we use numpy instead, as we did before.  The changes required are minimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cpu_parallel_monte_carlo_numpy:\n",
    "    \"\"\"\n",
    "    A class containing CPU based parallelization of the serial monte carlo\n",
    "    \n",
    "    inputs:\n",
    "        f    :    function; the moment to be estimated\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        \n",
    "    \"\"\"\n",
    "    # Define initial characteristics\n",
    "    def __init__(self, f, g, M, X, workers):\n",
    "        if M%workers is not 0:\n",
    "            raise ValueError('The vector of points must be evenly divisible'\n",
    "                             + ' by the number of workers.')\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "        self.M = M\n",
    "        self.X = X\n",
    "        self.workers = workers\n",
    "        self.queue = mp.Queue()\n",
    "        self.N =  X.shape[0]\n",
    "        self.points = np.random.rand(M, N)*(np.array([X[:, 1], ]*M) \n",
    "                                           - np.array([X[:, 0], ]*M))\\\n",
    "            +  np.array([X[:, 0], ]*M)\n",
    "        self.V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "    # Define functions that can be parallelized\n",
    "    def partial_sum(self, process):\n",
    "        \"\"\"\n",
    "        A function that will calculate the partial sum for the monte carlo\n",
    "        mean.\n",
    "        \n",
    "        inputs:\n",
    "            process   :    int; the process number\n",
    "\n",
    "        \"\"\"\n",
    "        # Unpack some info\n",
    "        K = int(self.M/self.workers)\n",
    "        partial_points = self.points[process*K:(process + 1)*K, :]\n",
    "\n",
    "        # Calculate the sum as a dot product\n",
    "        self.queue.put(np.dot(self.f(partial_points), self.g(partial_points, X)))\n",
    "    \n",
    "    def partial_var(self, process):\n",
    "        \"\"\"\n",
    "        A function that will calculate the partial sum for\n",
    "        the monte carlo variance.\n",
    "        \n",
    "        inputs:\n",
    "            process   :    int; the process number\n",
    "\n",
    "        \"\"\"\n",
    "        # Unpack some info\n",
    "        K = int(self.M/self.workers)\n",
    "        partial_points = self.points[process*K:(process + 1)*K, :]\n",
    "        \n",
    "        # Calculate the variance\n",
    "        self.queue.put(np.linalg.norm(self.f(partial_points)*self.g(partial_points, X) - self.mean/self.V)**2)\n",
    "\n",
    "    def mc_mean(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the mean by parallel montecarlo.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create a list of processes to run\n",
    "        processes = [mp.Process(target=self.partial_sum,\n",
    "                                kwargs=dict(process=i))\n",
    "                     for i in range(0, self.workers)]\n",
    "        \n",
    "        # Run the processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        \n",
    "        # When the processes are done, exit\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # Retrieve results from the queue and calculate the mean\n",
    "        partial_sums = [self.queue.get() for p in processes]\n",
    "        self.mean = sum(partial_sums)*self.V/self.M\n",
    "        \n",
    "        return self.mean\n",
    "    \n",
    "    def mc_var(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the variance by parallel montecarlo.\n",
    "        \n",
    "        \"\"\"        \n",
    "        # Create a list of processes to run\n",
    "        processes = [mp.Process(target=self.partial_var,\n",
    "                                kwargs=dict(process=i))\n",
    "                     for i in range(0, self.workers)]\n",
    "        \n",
    "        # Run the processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        \n",
    "        # When the processes are done, exit\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # Retrieve results from the queue and calculate the mean\n",
    "        partial_sums = [self.queue.get() for p in processes]\n",
    "        self.var = sum(partial_sums)*self.V**2/(self.M*(self.M - 1))\n",
    "        \n",
    "        return self.var\n",
    "    \n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        A method to calculate monte carlo integral and variance of estimate.\n",
    "        \"\"\"\n",
    "        return (self.mc_mean(), self.mc_var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should again check that the answers are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f4ccdc876990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpu_parallel_monte_carlo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "workers = 2\n",
    "M = 1000\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "test = cpu_parallel_monte_carlo(f, g, M, X, workers)\n",
    "test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "\n",
    "print(monte_carlo_expectation_serial(f, g, M, X))\n",
    "print(monte_carlo_expectation_numpy(f_vec, g_vec, M, X))\n",
    "print((test.mc_mean(), test.mc_var()))\n",
    "print(test_numpy.output())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super.  Now we can look at speed and see how these four compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.91 ms per loop\n",
      "10000 loops, best of 3: 119 µs per loop\n",
      "10 loops, best of 3: 40.7 ms per loop\n",
      "10 loops, best of 3: 39.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# NOTE: only see gains when M >> 100000\n",
    "workers = 4\n",
    "M = 100\n",
    "N = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "test = cpu_parallel_monte_carlo(f, g, M, X, workers)\n",
    "test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "\n",
    "# We can also time these three side by side\n",
    "%timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "%timeit (test.mc_mean(), test.mc_var())\n",
    "%timeit test_numpy.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the numpy imlimentation is faster, but the parallel numpy version is WAYYY slower than the serial one, 35 times slower!  Let's see how this does for higher values of $M$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 100\n",
      "100 loops, best of 3: 1.9 ms per loop\n",
      "10000 loops, best of 3: 119 µs per loop\n",
      "10 loops, best of 3: 41.3 ms per loop\n",
      "10 loops, best of 3: 42.2 ms per loop\n",
      "M = 1000\n",
      "100 loops, best of 3: 19 ms per loop\n",
      "1000 loops, best of 3: 727 µs per loop\n",
      "10 loops, best of 3: 48.9 ms per loop\n",
      "10 loops, best of 3: 40.3 ms per loop\n",
      "M = 10000\n",
      "10 loops, best of 3: 192 ms per loop\n",
      "100 loops, best of 3: 6.6 ms per loop\n",
      "10 loops, best of 3: 137 ms per loop\n",
      "10 loops, best of 3: 40.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "for M in [100, 1000, 10000]:\n",
    "    print(\"M = %s\" %M)\n",
    "    test = cpu_parallel_monte_carlo(f, g, M, X, workers)\n",
    "    test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "    %timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "    %timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "    %timeit (test.mc_mean(), test.mc_var())\n",
    "    %timeit test_numpy.output()\n",
    "    del test\n",
    "    del test_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidence of how efficient NumPy is, even at 10000 observations we still don't have a speed up.  In fact, we won't get one until M >> 10000 (keep in mind this is different on different systems.  In fact, on my home computer it takes M > 100000, while on the raspberry pi it is M > 15000. Not sure if that is a compliment for the raspberry pi or an embarassment for my desktop...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 67.4 ms per loop\n",
      "10 loops, best of 3: 45.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "M = 100000\n",
    "test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "%timeit test_numpy.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That sums up our introduction to multiprocessing, but if you would like to learn more check out the <a href=\"https://docs.python.org/3.4/library/multiprocessing.html\" target=\"_blank\">documentation</a>.  There are neat functions like `Pool()` which automate much of what we did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Parallelization\n",
    "Now that we've seen how multiprocessing can help us, what about GPU parallelization?  In fact, what the heck is a GPU!?\n",
    "\n",
    "A **Graphics Processing Unit (GPU)** is very similar to a CPU.  When you play modern video games it takes a massive amount of calculation to render the grass blowing in the wind or explosions (this is a topic for a PhD dissertation).  These calculations are often very similar and very simple, as well as highly parallelizable.  Because of this, NVidia, a graphics card company, created a special processor designed simply to render computer graphics.  They essentially took a CPU, removed everything that wasn't absolutely necessary for calculation, and packed it with cores. And more cores, and more cores...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cpu_v_gpu.jpg\" width=\"500\" /img>\n",
    "Source: Nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modern, bottom shelf graphics card from NVidia (like the one in my home computer) retails for around 100€ and contains 512 cores.  512 cores!  Think about that: a core on a GPU costs less than 20 centimes.  This is just the bottom shelf.  For example I have space on my motherboard for three graphics cards and they can be wired up in parallel.  So I can easily invest another 200€ euros and triply my computing speed. If we wanted to go to the top of the line NVidia scientific computing cards, we could get a Tesla K80, which offers 4992 cores PER CARD at 8.74 terraflops on single precission floating point arithmetic.  That's 8.74 million million operations per second!  To put this into context, in the year 2000 the fastest computer in the world was the IBM ASCI White... its peakspead was 7.226 terraflops.  So a single GPU is faster than the fastest computer in the world was when we entered the new millenium.  Wiring three of these bad boys up in parallel would create a desktop computer that could carry out 764 times the number of calculations per second as the IBM Deep Blue supercomputer that beat Gary Kasporov.  However, the cost of the cores is higher, at about 84 centimes per core.\n",
    "\n",
    "Nerding out completed, let's return to the task at hand.  Actually working with a GPU can be quite difficult.  Because they desire to cram all of these cores onto a single chip, the GPU is quite dumb.  It can really only do very basic calculations and the CPU must do all of the higher level organizing.  It is even necessary to communicate with the GPU in a special language called CUDA.  Thankfully, there's a package for that.\n",
    "\n",
    "PyCUDA is a package that automates the hard work of working with the GPU, but it is still necessary to do some data management and write a **\"kernel\"** in C.  Don't be discouraged!  You only have to do this for the most low level calculations and you can often just re-use someone else's kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threading\n",
    "Possibly the most difficult part about using PyCUDA is keeping track of the threads.  You could have hundreds or even thousands of threads running simultaneously.  To keep thes all in place, there are several structures known as the `grid` and the `blocks`.\n",
    "\n",
    "A grid is a two dimensional matrix of blocks.\n",
    "\n",
    "A block is a three dimensional matrix of threads.\n",
    "\n",
    "You can break up your work however you would like across these objects, but we will do the simplest thing.  We will define a block that is simply a vector of length 512 (the number of cores on my GPU. You should change this on your computer.).  We'll then define a grid as a vector of length $\\frac{M}{512}$.  In this way we will release $\\frac{M}{512}$ waves of 512 threads.\n",
    "\n",
    "Each one of these threads is indexed by its threadId's and blockId's.  For our set up, there is only the x direction, but there could also be a y and z direction.  Here's a short example of a CUDA kernal that carries out an elementwise multiplication and sotres the result in a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void elementwise_multiply(float *a, float *b)\n",
    "    {\n",
    "\n",
    "        const int t = threadIdx.x + blockIdx.x * 512 ; //the thread identifier to reference the original index\n",
    "\n",
    "        a[t] *= b[t];\n",
    "        \n",
    "    }\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line that calculates the index in the vectors based on the threadId and blockId.  This is a typical way to find the index.\n",
    "\n",
    "PyCUDA precompiles this code into a function, which you must retrieve.  To run it, you have to carry out several steps:\n",
    "\n",
    "1. Define the function and thread structure.\n",
    "2. Allocate memory both on cpu and gpu.\n",
    "3. Transfer data to the gpu.\n",
    "4. Execute the function.\n",
    "5. Retrieve the solution.\n",
    "\n",
    "Let's see all this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  6.  6. ...,  6.  6.  6.]\n",
      "[ 6.  6.  6. ...,  6.  6.  6.]\n"
     ]
    }
   ],
   "source": [
    "### DEFINE THE FUNCTION\n",
    "# Define a function for the pairwise multiplicaiton\n",
    "gpu_elementwise = mod.get_function(\"elementwise_multiply\")\n",
    "\n",
    "def super_fast_elementwise_multiply(a, b, NUMBER_OF_BLOCKS,\n",
    "                                 THREADS_PER_BLOCK):\n",
    "    \"\"\"\n",
    "    A GPU implementation of a elementwise multiplicaiton of two vectors.\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        a,b    :    ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    ### ALLOCATE MEMORY OIN THE CPU\n",
    "    # Allocate memory to fill with solution\n",
    "    c = np.zeros(a.shape[0]).astype(np.float32)\n",
    "    \n",
    "    ### ALLOCATE MEMORY ON THE GPU\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    \n",
    "    ### TRANSFER DATA TO THE DEVICE\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "    \n",
    "    ### EXECUTE THE FUNCTION\n",
    "    # Carry out the multiplication\n",
    "    gpu_elementwise(a_gpu, b_gpu,\n",
    "                    block=(THREADS_PER_BLOCK, 1, 1),\n",
    "                    grid=(NUMBER_OF_BLOCKS, 1))\n",
    "    \n",
    "    ### RETRIEVE SOLUTION\n",
    "    # Retrieve data from host\n",
    "    cuda.memcpy_dtoh(c, a_gpu)\n",
    "    \n",
    "    # Free up the memory\n",
    "    del a_gpu\n",
    "    del b_gpu\n",
    "\n",
    "    return c\n",
    "\n",
    "# How big do you want your vectors?  They will be of length M*512\n",
    "M = 10\n",
    "M *= 512\n",
    "\n",
    "### DEFINE THE THREAD STRUCTURE\n",
    "# Define GPU size parameters\n",
    "NUMBER_OF_BLOCKS = int(M/512)\n",
    "THREADS_PER_BLOCK = 512\n",
    "\n",
    "# Generate some data\n",
    "A = np.ones((M)).astype(np.float32)*3\n",
    "B = np.ones((M)).astype(np.float32)*2\n",
    "\n",
    "# Calculate the elementwise multiplication\n",
    "C = super_fast_elementwise_multiply(A, B, NUMBER_OF_BLOCKS,\n",
    "                                 THREADS_PER_BLOCK)\n",
    "print(C)\n",
    "print(A*B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Elements: 512000\n",
      "1000 loops, best of 3: 426 µs per loop\n",
      "100 loops, best of 3: 2.79 ms per loop\n",
      "\n",
      "Number of Elements: 5120000\n",
      "100 loops, best of 3: 5.76 ms per loop\n",
      "100 loops, best of 3: 17 ms per loop\n",
      "\n",
      "Number of Elements: 51200000\n",
      "10 loops, best of 3: 59.7 ms per loop\n",
      "10 loops, best of 3: 169 ms per loop\n",
      "\n",
      "Number of Elements: 102400000\n",
      "10 loops, best of 3: 121 ms per loop\n",
      "1 loops, best of 3: 337 ms per loop\n"
     ]
    }
   ],
   "source": [
    "#Success!  Now let's see how fast it is...\n",
    "# I hate to say it, but you'll run out of memory\n",
    "# before this speeds up your code!!!\n",
    "for M in [1000, 10000, 100000, 200000]:\n",
    "    M *= 512\n",
    "\n",
    "    # Define GPU size parameters\n",
    "    NUMBER_OF_BLOCKS = int(M/512)\n",
    "    THREADS_PER_BLOCK = 512\n",
    "\n",
    "    # Generate some data\n",
    "    a = np.ones((M)).astype(np.float32)*3\n",
    "    b = np.ones((M)).astype(np.float32)*2\n",
    "\n",
    "    print(\"\\nNumber of Elements: %s\" % M)\n",
    "    %timeit a*b\n",
    "    %timeit super_fast_elementwise_multiply(a, b, NUMBER_OF_BLOCKS, THREADS_PER_BLOCK)\n",
    "    \n",
    "    # Free up memory\n",
    "    del a\n",
    "    del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this doesn't give us much of a speed up.  This is caused by the high latency in data transfer to the GPU, as well as the efficiency of NumPy.  **Latency** is synonomous with lag or sluggishness.  When we transfer data to the GPU our calculation gets bogged down by the data transfer onto the device.  In order to benefit, we need to do more calculation and less transfer.\n",
    "\n",
    "One way we can take a step in this direction is if we wanted to do a dot product on the GPU.  The CUDA kernel below uses a pairwise summation to calculate the dot product.  It procedes in two steps:\n",
    "\n",
    "1. Carry out the elementwise multiplication.  When done, `__syncthreads();` tells CUDA to wait until all of the multiplications are terminated.\n",
    "\n",
    "2. Do a <a href=\"https://en.wikipedia.org/wiki/Pairwise_summation\" target=\"_blank\">pairwise summation</a>.  When complete, use `atomicAdd` to add the result for the block to the total sum.\n",
    "\n",
    "Note the `atomicAdd`.  We have not discussed this, but transferring data across threads is complicated.  However, since we are only adding to `c`, we don't have to worry too much.  The atomic method just adds to the object no matter the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void dot_prod(float *a, float *b, float *c)\n",
    "    {\n",
    "        // Define a shared object to fill with the elementwise mult\n",
    "        // NOTE: This can't be dynamic, so what I'll do is just fill\n",
    "        // a with the solution.  That would allow for dynamic shape\n",
    "        // NOTE: That won't work, as the pairwise sum is based on the\n",
    "        // block level thread id's\n",
    "        // ***** The number 512 must be changed to optimize on a specific\n",
    "        // machine.  Set equal number of cores.\n",
    "        __shared__ float temp[512];\n",
    "\n",
    "        //the thread identifier to reference the original index\n",
    "        const int t = threadIdx.x + blockIdx.x * blockDim.x ;\n",
    "\n",
    "        //the number of elements in a block to pairwise sum\n",
    "        int n = 256;\n",
    "        temp[threadIdx.x] = a[t]*b[t];\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        //now pairwise sum the vector temp, store in temp, then output to c\n",
    "        while (n != 0)\n",
    "        {\n",
    "            if (threadIdx.x < n)\n",
    "                temp[threadIdx.x] += temp[threadIdx.x+n];\n",
    "                __syncthreads();\n",
    "            n /= 2;\n",
    "        }\n",
    "\n",
    "        //add the total to the sum only once per block\n",
    "        if (0 == threadIdx.x)\n",
    "            atomicAdd(c,temp[0]);\n",
    "    }\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30720.]\n",
      "30720.0\n"
     ]
    }
   ],
   "source": [
    "# Define a function for the pairwise multiplicaiton\n",
    "gpu_dot = mod.get_function(\"dot_prod\")\n",
    "\n",
    "def super_fast_dot_product(a, b, NUMBER_OF_BLOCKS, THREADS_PER_BLOCK):\n",
    "    \"\"\"\n",
    "    A GPU implementation of a elementwise multiplicaiton of two vectors.\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        a,b    :    ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    # Allocate memory to fill with solution\n",
    "    c = np.zeros(1).astype(np.float32)\n",
    "\n",
    "    # Allocate memory on the gpu\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "    \n",
    "    # Transfer data to the device\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "    cuda.memcpy_htod(c_gpu, c)\n",
    "    \n",
    "    # Carry out the multiplication\n",
    "    gpu_dot(a_gpu, b_gpu, c_gpu,\n",
    "                    block=(THREADS_PER_BLOCK, 1, 1),\n",
    "                    grid=(NUMBER_OF_BLOCKS, 1))\n",
    "    \n",
    "    # Retrieve data from host\n",
    "    cuda.memcpy_dtoh(c, c_gpu)\n",
    "    \n",
    "    return c\n",
    "\n",
    "threads = 512\n",
    "# How big do you want your vectors?  They will be of length M*512\n",
    "M = 10\n",
    "M *= threads\n",
    "\n",
    "# Define GPU size parameters\n",
    "NUMBER_OF_BLOCKS = int(M/threads)\n",
    "THREADS_PER_BLOCK = threads\n",
    "\n",
    "# Generate some data\n",
    "A = np.ones((M)).astype(np.float32)*3\n",
    "B = np.ones((M)).astype(np.float32)*2\n",
    "\n",
    "# Calculate the elementwise multiplication\n",
    "C = super_fast_dot_product(A, B, NUMBER_OF_BLOCKS,\n",
    "                                 THREADS_PER_BLOCK)\n",
    "print(C)\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Elements: 512000\n",
      "10000 loops, best of 3: 182 µs per loop\n",
      "1000 loops, best of 3: 1.21 ms per loop\n",
      "\n",
      "Number of Elements: 5120000\n",
      "100 loops, best of 3: 3.5 ms per loop\n",
      "100 loops, best of 3: 10.1 ms per loop\n",
      "\n",
      "Number of Elements: 51200000\n",
      "10 loops, best of 3: 33 ms per loop\n",
      "10 loops, best of 3: 94.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "#Success again!\n",
    "# I hate to say it, but you'll run out of memory\n",
    "# before this speeds up your code!!!\n",
    "for M in [1000, 10000, 100000]:\n",
    "    M *= 512\n",
    "\n",
    "    # Define GPU size parameters\n",
    "    NUMBER_OF_BLOCKS = int(M/512)\n",
    "    THREADS_PER_BLOCK = 512\n",
    "\n",
    "    # Generate some data\n",
    "    a = np.ones((M)).astype(np.float32)*3\n",
    "    b = np.ones((M)).astype(np.float32)*2\n",
    "\n",
    "    print(\"\\nNumber of Elements: %s\" % M)\n",
    "    %timeit np.dot(a,b)\n",
    "    %timeit super_fast_dot_product(a, b, NUMBER_OF_BLOCKS, THREADS_PER_BLOCK)\n",
    "    \n",
    "    # Free up memory\n",
    "    del a\n",
    "    del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are still not getting the speed up we were hoping for.  However, if we also can parallelize our moment or distribution function, we could get a speed up.  Finally, let's write a kernel to speed up our monte carlo calculation.\n",
    "\n",
    "**NOTE:** There is an error in this version that I have created while editing and can't seem to find... bonus points to anyone who can point it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\"\n",
    "    __global__ void sum1(float *a, float *b, float *c)\n",
    "    {\n",
    "        // Objects to store pairwise sums\n",
    "        //__shared__ float temp2[%(MATRIX_SIZE)s];\n",
    "        __shared__ float temp2[256];\n",
    "\n",
    "        // 2D Thread ID to calculate L2 norm\n",
    "        // index to entry in a\n",
    "        const int ida = threadIdx.y + threadIdx.x*blockDim.y + blockIdx.x*blockDim.y*blockDim.x;\n",
    "\n",
    "        // The block level id over matrix temp1\n",
    "        //const int block_tx = threadIdx.y + threadIdx.x*blockDim.y;\n",
    "\n",
    "        // Index to entry in b\n",
    "        const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "        // Constants for pairwise sum\n",
    "        int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "        int k = blockDim.y / 2; //the number of columns in a block to pairwise sum\n",
    "\n",
    "        // Square the entries in the sum\n",
    "        a[ida] *= a[ida];\n",
    "\n",
    "        // Carry out pairwise summation of squared entries along row\n",
    "        // to calculate the L2 norm\n",
    "        while (k != 0)\n",
    "        {\n",
    "            if (threadIdx.y < k)\n",
    "                a[ida] += a[ida + k];\n",
    "            __syncthreads();\n",
    "            k /= 2;\n",
    "        }\n",
    "\n",
    "        // Carry out the elementwise multiplication\n",
    "        // NOTE: only on left most row entry\n",
    "        if (0 == threadIdx.y)\n",
    "            temp2[threadIdx.x] = a[ida]*b[idb];\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "        while (n != 0)\n",
    "        {\n",
    "            if (threadIdx.x < n)\n",
    "                if (0 == threadIdx.y)\n",
    "                    temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "            __syncthreads();\n",
    "            n /= 2;\n",
    "        }\n",
    "        //add the total to the sum only once per block\n",
    "        if (0 == threadIdx.x)\n",
    "            if (0 == threadIdx.y)\n",
    "                atomicAdd(c,temp2[0]);\n",
    "    }\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.67277291379281345, 7.0744866043010118e-05)\n",
      "(array([ 0.36519364], dtype=float32), array([ 0.00680973]))\n"
     ]
    }
   ],
   "source": [
    "class GPU_parallel_L2_monte_carlo_numpy:\n",
    "    \"\"\"\n",
    "    A class containing GPU based parallelization of monte carlo\n",
    "    \n",
    "    inputs:\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        cores:    scalar; number of cuda cores on gpu \n",
    "        \n",
    "    \"\"\"\n",
    "    # Define initial characteristics\n",
    "    def __init__(self, g, M, X, cores):\n",
    "        # NOTE: This is no longer necessary.  We've incorporated it into the kernel\n",
    "        #self.f = f\n",
    "        # NOTE: We could get more speed from using the fact that\n",
    "        # g is constant over x, but for \n",
    "        # comparability issues we'll keep this\n",
    "        self.g = g\n",
    "        self.M = M\n",
    "        self.X = X\n",
    "        self.N =  X.shape[0]\n",
    "        self.points = np.random.rand(M, N)*(np.array([X[:, 1], ]*M) \n",
    "                                           - np.array([X[:, 0], ]*M))\\\n",
    "            +  np.array([X[:, 0], ]*M)\n",
    "        self.V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "        \n",
    "        # Define GPU size parameters\n",
    "        self.cores = cores\n",
    "        if self.cores % N is not 0:\n",
    "            raise ValueError('The number of cores is not divisible by '\n",
    "                             + 'the dimension N.')\n",
    "        self.threadsX = int(self.cores/N)\n",
    "        if self.M % self.threadsX is not 0:\n",
    "            raise ValueError('The number of observations is not divisible '\n",
    "                               + 'by the number of threads per block.')\n",
    "        self.NUMBER_OF_BLOCKS = int(self.M/self.cores)\n",
    "        self.THREADS_X_PER_BLOCK = self.threadsX\n",
    "        self.THREADS_Y_PER_BLOCK = self.N\n",
    "\n",
    "        # Define a string object containing the kernel\n",
    "        kernel_code = \"\"\"\n",
    "            #include <stdio.h>\n",
    "            __global__ void sum1(float *a, float *b, float *c)\n",
    "            {\n",
    "                // Objects to store pairwise sums\n",
    "                __shared__ float temp2[256];\n",
    "                \n",
    "                // 2D Thread ID to calculate L2 norm\n",
    "                // index to entry in a\n",
    "                const int ida = threadIdx.y + threadIdx.x*blockDim.y + blockIdx.x*blockDim.y*blockDim.x;\n",
    "\n",
    "                // Index to entry in b\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                // Constants for pairwise sum\n",
    "                int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "                int k = blockDim.y / 2; //the number of columns in a block to pairwise sum\n",
    "\n",
    "                // Square the entries in the sum\n",
    "                a[ida] *= a[ida];\n",
    "\n",
    "                // Carry out pairwise summation of squared entries along row\n",
    "                // to calculate the L2 norm\n",
    "                while (k != 0)\n",
    "                {\n",
    "                    if (threadIdx.y < k)\n",
    "                        a[ida] += a[ida + k];\n",
    "                    __syncthreads();\n",
    "                    k /= 2;\n",
    "                }\n",
    "\n",
    "                // Carry out the elementwise multiplication\n",
    "                // NOTE: only on left most row entry\n",
    "                if (0 == threadIdx.y)\n",
    "                    temp2[threadIdx.x] = a[ida]*b[idb];\n",
    "\n",
    "                __syncthreads();\n",
    "\n",
    "                //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "                while (n != 0)\n",
    "                {\n",
    "                    if (threadIdx.x < n)\n",
    "                        if (0 == threadIdx.y)\n",
    "                            temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "                    __syncthreads();\n",
    "                    n /= 2;\n",
    "                }\n",
    "                //add the total to the sum only once per block\n",
    "                if (0 == threadIdx.x)\n",
    "                    if (0 == threadIdx.y)\n",
    "                        atomicAdd(c,temp2[0]);\n",
    "            }\n",
    "\n",
    "\n",
    "            __global__ void var1(float *a, float *b, float *c, float *d)\n",
    "            {\n",
    "                // Objects to store pairwise sums\n",
    "                //__shared__ float temp2[%(MATRIX_SIZE)s];\n",
    "                __shared__ float temp2[256];\n",
    "\n",
    "                // 2D Thread ID to calculate L2 norm\n",
    "                // index to entry in a\n",
    "                const int ida = threadIdx.y + threadIdx.x*blockDim.y + blockIdx.x*blockDim.y*blockDim.x;\n",
    "                \n",
    "                // Index to entry in b\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                // Constants for pairwise sum\n",
    "                int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "                int k = blockDim.y / 2; //the number of columns in a block to pairwise sum\n",
    "\n",
    "                // Square the entries in the sum\n",
    "                a[ida] *= a[ida];\n",
    "\n",
    "                // Carry out pairwise summation of squared entries along row\n",
    "                // to calculate the L2 norm\n",
    "                while (k != 0)\n",
    "                {\n",
    "                    if (threadIdx.y < k)\n",
    "                        a[ida] += a[ida + k];\n",
    "                    __syncthreads();\n",
    "                    k /= 2;\n",
    "                }\n",
    "\n",
    "                // Carry out the elementwise multiplication\n",
    "                // NOTE: only on left most row entry\n",
    "                printf(\"%f \\\\n\", d[0]);\n",
    "                if (0 == threadIdx.y)\n",
    "                    temp2[threadIdx.x] = a[ida]*b[idb];\n",
    "                    temp2[threadIdx.x] -= d[0];\n",
    "                    temp2[threadIdx.x] *= temp2[threadIdx.x];\n",
    "\n",
    "                __syncthreads();\n",
    "\n",
    "                //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "                while (n != 0)\n",
    "                {\n",
    "                    if (threadIdx.x < n)\n",
    "                        if (0 == threadIdx.y)\n",
    "                            temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "                    __syncthreads();\n",
    "                    n /= 2;\n",
    "                }\n",
    "\n",
    "                //add the total to the sum only once per block\n",
    "                if (0 == threadIdx.x)\n",
    "                    if (0 == threadIdx.y)\n",
    "                        atomicAdd(c,temp2[0]);\n",
    "            }\n",
    "\n",
    "            \"\"\"\n",
    "        # Pass to PyCUDA the size of the matrix\n",
    "        #kernel_code = kernel_code % {\n",
    "        #    'MATRIX_SIZE': self.THREADS_X_PER_BLOCK,\n",
    "        #    }\n",
    "\n",
    "        # compile the kernel code \n",
    "        mod = SourceModule(kernel_code)\n",
    "        \n",
    "        # Define a function for the parallel sum\n",
    "        self.gpu_sum1 = mod.get_function(\"sum1\")\n",
    "        self.gpu_var1 = mod.get_function(\"var1\")\n",
    "\n",
    "    def mean1(self):\n",
    "        \"\"\"\n",
    "        A GPU implementation of monte carlo mean.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the probability of the observations in serial\n",
    "        # NOTE: Even more speed up available here by parallelization\n",
    "        self.g_of_points = self.g(self.points, self.X).astype(np.float32)\n",
    "        \n",
    "        # Allocate memory to fill with solution\n",
    "        sum1 = np.zeros(1).astype(np.float32)\n",
    "\n",
    "        \n",
    "        # Allocate memory on the gpu\n",
    "        a_gpu = cuda.mem_alloc(self.points.astype(np.float32).nbytes)\n",
    "        b_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        c_gpu = cuda.mem_alloc(sum1.nbytes)\n",
    "\n",
    "        # Transfer data to the device\n",
    "        cuda.memcpy_htod(a_gpu, self.points.astype(np.float32))\n",
    "        cuda.memcpy_htod(b_gpu, self.g_of_points.astype(np.float32))\n",
    "        cuda.memcpy_htod(c_gpu, sum1)\n",
    "\n",
    "        # Carry out the multiplication\n",
    "        self.gpu_sum1(a_gpu, b_gpu, c_gpu,\n",
    "                        block=(self.THREADS_X_PER_BLOCK,\n",
    "                               self.THREADS_Y_PER_BLOCK, 1),\n",
    "                        grid=(self.NUMBER_OF_BLOCKS, 1))\n",
    "\n",
    "        # Retrieve data from host\n",
    "        cuda.memcpy_dtoh(sum1, c_gpu)\n",
    "\n",
    "        # Calculate the mean\n",
    "        self.mean = sum1*self.V/self.M\n",
    "        \n",
    "        # Clean up memory\n",
    "        del a_gpu, b_gpu, c_gpu\n",
    "        \n",
    "        return self.mean\n",
    "    \n",
    "    def var1(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the variance by gpu parallel montecarlo.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Calculate the probability of the observations in serial\n",
    "        # NOTE: Even more speed up available here by parallelization\n",
    "        # NOTE: I saved this information from previous calculation\n",
    "        #g_of_points = self.g(self.points, self.X)\n",
    "        # Calculate parameter\n",
    "        d = np.array((self.mean/self.V), dtype=np.float32)\n",
    "        \n",
    "        # Allocate memory to fill with solution\n",
    "        var1 = np.zeros(1).astype(np.float32)\n",
    "\n",
    "        # Allocate memory on the gpu\n",
    "        a_gpu = cuda.mem_alloc(self.points.astype(np.float32).nbytes)\n",
    "        b_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        c_gpu = cuda.mem_alloc(var1.nbytes)\n",
    "        d_gpu = cuda.mem_alloc(d.nbytes)\n",
    "\n",
    "        # Transfer data to the device\n",
    "        cuda.memcpy_htod(a_gpu, self.points.astype(np.float32))\n",
    "        cuda.memcpy_htod(b_gpu, self.g_of_points.astype(np.float32))\n",
    "        cuda.memcpy_htod(c_gpu, var1)\n",
    "        cuda.memcpy_htod(d_gpu, d)\n",
    "\n",
    "        # Carry out the multiplication\n",
    "        self.gpu_var1(a_gpu, b_gpu, c_gpu, d_gpu,\n",
    "                        block=(self.THREADS_X_PER_BLOCK,\n",
    "                               self.THREADS_Y_PER_BLOCK, 1),\n",
    "                        grid=(self.NUMBER_OF_BLOCKS, 1))\n",
    "\n",
    "        # Retrieve data from host\n",
    "        cuda.memcpy_dtoh(var1, c_gpu)\n",
    "        \n",
    "        # Calculate the variance\n",
    "        self.var = var1*self.V**2/(self.M*(self.M - 1))\n",
    "        \n",
    "        # Clean up memory\n",
    "        del a_gpu, b_gpu, c_gpu, d_gpu\n",
    "        return self.var\n",
    "    \n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        A method to calculate monte carlo integral and variance of estimate.\n",
    "        \"\"\"\n",
    "        return (self.mean1(), self.var1())\n",
    "\n",
    "\n",
    "### Define parameterization\n",
    "# Dimension of the random variable\n",
    "N = 2\n",
    "cores = 512\n",
    "# #arameter controlling number of rows\n",
    "# NOTE: defined this way so each block is completely rows; no rows\n",
    "# are split across blocks\n",
    "threadsX = int(cores/N)\n",
    "# Parameter controlling number of observations\n",
    "# They will be of length M*512\n",
    "M = 5\n",
    "M *= cores\n",
    "\n",
    "# Define GPU size parameters\n",
    "#NUMBER_OF_BLOCKS = int(M/threadsX)\n",
    "#THREADS_X_PER_BLOCK = threadsX\n",
    "#THREADS_Y_PER_BLOCK = N\n",
    "\n",
    "# Generate some data\n",
    "#A = np.ones((M)).astype(np.float32)*3\n",
    "#B = np.ones((M)).astype(np.float32)*2\n",
    "\n",
    "workers = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "\n",
    "# Calculate the elementwise multiplication\n",
    "test = GPU_parallel_L2_monte_carlo_numpy(g_vec, M, X, cores)\n",
    "test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "print(test_numpy.output())\n",
    "print(test.output())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As I said, while editing I introduced an error.**\n",
    "\n",
    "Ignoring my error, this function is actually even slower than before!  Why?  The reason has to do with the thread structure.\n",
    "\n",
    "When we define our function over a two dimensional block of threads, we ignore the problem of idle threads.  In particular, we initially use all of our threads in the calculation of the norm, but then only use a single column of threads to calculate the sum.  This implies that after the norm is calculated we leave $512\\frac{N-1}{N}$ threads idle.  To address this we can split the two functions and redistribute the threads.  We do this by de\"fining two seperate functions on the GPU for the norm and the moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPU_parallel_L2_monte_carlo_block:\n",
    "    \"\"\"\n",
    "    A class containing GPU based parallelization of monte carlo\n",
    "\n",
    "    inputs:\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        cores:    scalar; number of cuda cores on gpu\n",
    "\n",
    "    \"\"\"\n",
    "    # Define initial characteristics\n",
    "    def __init__(self, g, M, X, cores):\n",
    "        # NOTE: This is no longer necessary.  We've incorporated it into the kernel\n",
    "        #self.f = f\n",
    "        # NOTE: We could get more speed from using the fact that\n",
    "        # g is constant over x, but for\n",
    "        # comparability issues we'll keep this\n",
    "        self.g = g\n",
    "        self.M = M\n",
    "        self.X = X\n",
    "        self.N =  X.shape[0]\n",
    "        self.points = np.random.rand(M, N)*(np.array([X[:, 1], ]*M)\n",
    "                                           - np.array([X[:, 0], ]*M))\\\n",
    "            +  np.array([X[:, 0], ]*M)\n",
    "        self.V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "        # Define GPU size parameters\n",
    "        self.cores = cores\n",
    "        if self.cores % self.N is not 0:\n",
    "            raise ValueError('The number of cores is not divisible by '\n",
    "                             + 'the dimension N.')\n",
    "        self.threadsX = int(self.cores/self.N)\n",
    "        if self.M % self.threadsX is not 0:\n",
    "            raise ValueError('The number of observations is not divisible '\n",
    "                               + 'by the number of threads per block.')\n",
    "        self.NUMBER_OF_BLOCKS = int(self.M*self.N/self.cores)\n",
    "        self.THREADS_X_PER_BLOCK = self.threadsX\n",
    "        self.THREADS_Y_PER_BLOCK = self.N\n",
    "\n",
    "        # Define a string object containing the kernel\n",
    "        kernel_code = \"\"\"\n",
    "            #include <stdio.h>\n",
    "            __global__ void L2_norm(float *a, float *b)\n",
    "            {\n",
    "                // 2D Thread ID to calculate L2 norm\n",
    "                // index to entry in a\n",
    "                const int ida = threadIdx.y + threadIdx.x*blockDim.y + blockIdx.x*blockDim.y*blockDim.x;\n",
    "                // Index to entry in b\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                int k = blockDim.y / 2; //the number of columns in a block to pairwise sum\n",
    "\n",
    "                // Square the entries in the sum\n",
    "                a[ida] *= a[ida];\n",
    "\n",
    "                // Carry out pairwise summation of squared entries along row\n",
    "                // to calculate the L2 norm\n",
    "                while (k != 0)\n",
    "                {\n",
    "                    if (threadIdx.y < k)\n",
    "                        a[ida] += a[ida + k];\n",
    "                    __syncthreads();\n",
    "                    k /= 2;\n",
    "                }\n",
    "\n",
    "                // Output to b\n",
    "                if (0 == threadIdx.y)\n",
    "                {\n",
    "                    b[idb] = a[ida];\n",
    "                }\n",
    "            }\n",
    "\n",
    "            __global__ void sum1(float *a, float *b, float *c)\n",
    "            {\n",
    "                // Object to store pairwise sum\n",
    "                __shared__ float temp2[512];\n",
    "\n",
    "                // Index to entry\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                // Constants for pairwise sum\n",
    "                int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "\n",
    "                // Carry out the elementwise multiplication\n",
    "                temp2[threadIdx.x] = a[idb]*b[idb];\n",
    "\n",
    "                __syncthreads();\n",
    "\n",
    "                //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "                while (n != 0)\n",
    "                {\n",
    "                    if (threadIdx.x < n)\n",
    "                        temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "                    __syncthreads();\n",
    "                    n /= 2;\n",
    "                }\n",
    "                //add the total to the sum only once per block\n",
    "                if (0 == threadIdx.x)\n",
    "                    atomicAdd(c,temp2[0]);\n",
    "            }\n",
    "\n",
    "\n",
    "            __global__ void var1(float *a, float *b, float *c, float *d)\n",
    "            {\n",
    "                // Objects to store pairwise sums\n",
    "                __shared__ float temp2[512];\n",
    "\n",
    "                // Index to entry in b\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                // Constants for pairwise sum\n",
    "                int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "\n",
    "                // Carry out the elementwise multiplication\n",
    "                // NOTE: only on left most row entry\n",
    "                //printf(\"%f \\\\n\", d[0]);\n",
    "                if (0 == threadIdx.y)\n",
    "                    temp2[threadIdx.x] = a[idb]*b[idb];\n",
    "                    temp2[threadIdx.x] -= d[0];\n",
    "                    temp2[threadIdx.x] *= temp2[threadIdx.x];\n",
    "\n",
    "                __syncthreads();\n",
    "\n",
    "                //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "                while (n != 0)\n",
    "                {\n",
    "                    if (threadIdx.x < n)\n",
    "                        temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "                    __syncthreads();\n",
    "                    n /= 2;\n",
    "                }\n",
    "\n",
    "                //add the total to the sum only once per block\n",
    "                if (0 == threadIdx.x)\n",
    "                    atomicAdd(c,temp2[0]);\n",
    "            }\n",
    "\n",
    "            \"\"\"\n",
    "        # Pass to PyCUDA the size of the matrix\n",
    "        #kernel_code = kernel_code % {\n",
    "        #    'MATRIX_SIZE': self.THREADS_X_PER_BLOCK,\n",
    "        #    }\n",
    "\n",
    "        # compile the kernel code\n",
    "        mod = SourceModule(kernel_code)\n",
    "\n",
    "        # Define a function for the parallel sum\n",
    "        self.gpu_sum1 = mod.get_function(\"sum1\")\n",
    "        self.gpu_var1 = mod.get_function(\"var1\")\n",
    "        self.gpu_L2 = mod.get_function(\"L2_norm\")\n",
    "\n",
    "    def mean1(self):\n",
    "        \"\"\"\n",
    "        A GPU implementation of monte carlo mean.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the probability of the observations in serial\n",
    "        # NOTE: Even more speed up available here by parallelization\n",
    "        self.g_of_points = self.g(self.points, self.X).astype(np.float32)\n",
    "\n",
    "        # Allocate memory to fill with solution\n",
    "        sum1 = np.zeros(1).astype(np.float32)\n",
    "\n",
    "        # Allocate memory on the gpu\n",
    "        a_gpu = cuda.mem_alloc(self.points.astype(np.float32).nbytes)\n",
    "        a_temp_gpu = cuda.mem_alloc(self.g_of_points.nbytes)\n",
    "        b_gpu = cuda.mem_alloc(self.g_of_points.nbytes)\n",
    "        c_gpu = cuda.mem_alloc(sum1.nbytes)\n",
    "\n",
    "        # Transfer data to the device\n",
    "        cuda.memcpy_htod(a_gpu, self.points.astype(np.float32))\n",
    "        cuda.memcpy_htod(a_temp_gpu, np.zeros(self.M).astype(np.float32))\n",
    "        cuda.memcpy_htod(b_gpu, self.g_of_points)\n",
    "        cuda.memcpy_htod(c_gpu, sum1)\n",
    "\n",
    "        # Carry out the multiplication\n",
    "        self.gpu_L2(a_gpu, a_temp_gpu, block=(self.THREADS_X_PER_BLOCK,\n",
    "                                              self.THREADS_Y_PER_BLOCK, 1),\n",
    "                    grid=(self.NUMBER_OF_BLOCKS, 1))\n",
    "        self.gpu_sum1(a_temp_gpu, b_gpu, c_gpu,\n",
    "                      block=(self.cores, 1, 1),\n",
    "                      grid=(int(self.NUMBER_OF_BLOCKS/self.N), 1))\n",
    "\n",
    "        # Retrieve data from host\n",
    "        cuda.memcpy_dtoh(sum1, c_gpu)\n",
    "\n",
    "        # Calculate the mean\n",
    "        self.mean = np.copy(sum1)*self.V/self.M\n",
    "\n",
    "        # Clean up memory\n",
    "        del a_gpu, a_temp_gpu, b_gpu, c_gpu\n",
    "\n",
    "        return self.mean\n",
    "\n",
    "    def var1(self):\n",
    "        \"\"\"\n",
    "        A method to calculate the variance by gpu parallel montecarlo.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the probability of the observations in serial\n",
    "        # NOTE: Even more speed up available here by parallelization\n",
    "        # NOTE: I saved this information from previous calculation\n",
    "        #g_of_points = self.g(self.points, self.X)\n",
    "        # Calculate parameter\n",
    "        d = np.array((self.mean/self.V), dtype=np.float32)\n",
    "\n",
    "        # Allocate memory to fill with solution\n",
    "        var1 = np.zeros(1).astype(np.float32)\n",
    "\n",
    "        # Allocate memory on the gpu\n",
    "        a_gpu = cuda.mem_alloc(self.points.astype(np.float32).nbytes)\n",
    "        a_temp_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        b_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        c_gpu = cuda.mem_alloc(var1.nbytes)\n",
    "        d_gpu = cuda.mem_alloc(d.nbytes)\n",
    "\n",
    "        # Transfer data to the device\n",
    "        cuda.memcpy_htod(a_gpu, self.points.astype(np.float32))\n",
    "        cuda.memcpy_htod(b_gpu, self.g_of_points.astype(np.float32))\n",
    "        cuda.memcpy_htod(c_gpu, var1)\n",
    "        cuda.memcpy_htod(d_gpu, d)\n",
    "\n",
    "        # Carry out the multiplication\n",
    "        self.gpu_L2(a_gpu, a_temp_gpu, block=(self.THREADS_X_PER_BLOCK,\n",
    "                                              self.THREADS_Y_PER_BLOCK, 1),\n",
    "                    grid=(self.NUMBER_OF_BLOCKS, 1))\n",
    "        self.gpu_var1(a_temp_gpu, b_gpu, c_gpu, d_gpu,\n",
    "                      block=(self.cores, 1, 1),\n",
    "                      grid=(int(self.NUMBER_OF_BLOCKS/self.N), 1))\n",
    "\n",
    "        # Retrieve data from host\n",
    "        cuda.memcpy_dtoh(var1, c_gpu)\n",
    "\n",
    "        # Calculate the variance\n",
    "        self.var = np.copy(var1)*self.V**2/(self.M*(self.M - 1))\n",
    "\n",
    "        # Clean up memory\n",
    "        del a_gpu, b_gpu, c_gpu, d_gpu\n",
    "        return self.var\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        A method to calculate monte carlo integral and variance of estimate.\n",
    "        \"\"\"\n",
    "        return (self.mean1(), self.var1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65008526445444514, 0.00039038503054202115)\n",
      "(array([ 0.66984808], dtype=float32), array([ 0.00034686]))\n"
     ]
    }
   ],
   "source": [
    "### Define parameterization\n",
    "# Dimension of the random variable\n",
    "N = 2\n",
    "cores = 512\n",
    "# #arameter controlling number of rows\n",
    "# NOTE: defined this way so each block is completely rows; no rows\n",
    "# are split across blocks\n",
    "threadsX = int(cores/N)\n",
    "# Parameter controlling number of observations\n",
    "# They will be of length M*512\n",
    "M = 2\n",
    "M *= threadsX\n",
    "\n",
    "# Define GPU size parameters\n",
    "#NUMBER_OF_BLOCKS = int(M/threadsX)\n",
    "#THREADS_X_PER_BLOCK = threadsX\n",
    "#THREADS_Y_PER_BLOCK = N\n",
    "\n",
    "# Generate some data\n",
    "#A = np.ones((M)).astype(np.float32)*3\n",
    "#B = np.ones((M)).astype(np.float32)*2\n",
    "\n",
    "workers = 2\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "\n",
    "# Calculate the elementwise multiplication\n",
    "test = GPU_parallel_L2_monte_carlo_block(g_vec, M, X, cores)\n",
    "test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "print(test_numpy.output())\n",
    "print(test.output())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "M = 512\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.050628662109375 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0007331371307373047 seconds \n",
      "\n",
      "0.672066684165 [ 0.70166504]\n",
      "0.000357086780394 [ 0.00034693]\n",
      "512 1024 1024\n",
      "512 2\n",
      "2 256 2\n",
      "512 512\n",
      "1.0 1.0\n",
      "\n",
      "\n",
      "M = 5120\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.04885745048522949 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0011625289916992188 seconds \n",
      "\n",
      "0.664581233453 [ 0.70603484]\n",
      "3.49983027665e-05 [  3.74081871e-05]\n",
      "5120 10240 10240\n",
      "512 20\n",
      "20 256 2\n",
      "5120 5120\n",
      "1.0 1.0\n",
      "\n",
      "\n",
      "M = 51200\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.051087379455566406 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0031862258911132812 seconds \n",
      "\n",
      "0.668189490004 [ 0.71707338]\n",
      "3.50057251041e-06 [  3.55969244e-06]\n",
      "51200 102400 102400\n",
      "512 200\n",
      "200 256 2\n",
      "51200 51200\n",
      "1.0 1.0\n",
      "\n",
      "\n",
      "M = 512000\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.08083868026733398 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.02205514907836914 seconds \n",
      "\n",
      "0.666878076079 [ 0.6936203]\n",
      "3.47104518107e-07 [  3.53021536e-07]\n",
      "512000 1024000 1024000\n",
      "512 2000\n",
      "2000 256 2\n",
      "512000 512000\n",
      "1.0 1.0\n",
      "\n",
      "\n",
      "M = 5120000\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.3504774570465088 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.2295088768005371 seconds \n",
      "\n",
      "0.666835827599 [ 0.69409995]\n",
      "3.47292112444e-08 [  3.53105614e-08]\n",
      "5120000 10240000 10240000\n",
      "512 20000\n",
      "20000 256 2\n",
      "5120000 5120000\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "N = 2\n",
    "cores = 512\n",
    "threadsX = int(cores/N)\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "# NOTE: For optimization reasons, M >= N => #observations/#dimensions >=1\n",
    "for M in [N, N*10, N*100, N*1000, N*10000]:\n",
    "    M *= threadsX\n",
    "    print(\"\\n\\nM = %s\" % M)\n",
    "    test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "    test_gpu = GPU_parallel_L2_monte_carlo_block(g_vec, M, X, cores)\n",
    "    #print(\"SERIAL NATIVE\")\n",
    "    #%timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "    #print(\"SERIAL NUMPY\")\n",
    "    #%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "    print(\"CPU PARALLEL NUMPY\")\n",
    "    start = time.time()\n",
    "    test_numpy.output()\n",
    "    print(\"Executed in %s seconds \\n\" % (time.time() - start))\n",
    "    print(\"GPU FULLY PARALLEL\")\n",
    "    start = time.time()\n",
    "    test_gpu.output()\n",
    "    print(\"Executed in %s seconds \\n\" % (time.time() - start))\n",
    "    print(test_numpy.mean, test_gpu.mean)\n",
    "    print(test_numpy.var, test_gpu.var)\n",
    "    print(M,test_gpu.M*test_gpu.N, test_gpu.THREADS_X_PER_BLOCK*test_gpu.THREADS_Y_PER_BLOCK*test_gpu.NUMBER_OF_BLOCKS)\n",
    "    print(test_gpu.THREADS_X_PER_BLOCK*test_gpu.THREADS_Y_PER_BLOCK, test_gpu.NUMBER_OF_BLOCKS)\n",
    "    print(test_gpu.NUMBER_OF_BLOCKS, test_gpu.THREADS_X_PER_BLOCK, test_gpu.THREADS_Y_PER_BLOCK)\n",
    "    print(test_numpy.M, test_gpu.M)\n",
    "    print(test_numpy.V, test_gpu.V)\n",
    "    del test_numpy\n",
    "    del test_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we speed this up by leaving the L2 on the gpu?  Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPU_parallel_L2_monte_carlo_block:\n",
    "    \"\"\"\n",
    "    A class containing GPU based parallelization of monte carlo\n",
    "\n",
    "    inputs:\n",
    "        g    :    function; the distribution function of x\n",
    "        M    :    scalar; the number of points to sample\n",
    "        X    :    ndarray; bounds in R^n for the support\n",
    "        cores:    scalar; number of cuda cores on gpu\n",
    "\n",
    "    \"\"\"\n",
    "    # Define initial characteristics\n",
    "    def __init__(self, g, M, X, cores):\n",
    "        # NOTE: This is no longer necessary.  We've incorporated it into the kernel\n",
    "        #self.f = f\n",
    "        # NOTE: We could get more speed from using the fact that\n",
    "        # g is constant over x, but for\n",
    "        # comparability issues we'll keep this\n",
    "        self.g = g\n",
    "        self.M = M\n",
    "        self.X = X\n",
    "        self.N =  X.shape[0]\n",
    "        self.points = np.random.rand(M, N)*(np.array([X[:, 1], ]*M)\n",
    "                                           - np.array([X[:, 0], ]*M))\\\n",
    "            +  np.array([X[:, 0], ]*M)\n",
    "        self.V = np.prod(np.abs(X[:,1] - X[:,0]))\n",
    "\n",
    "        # Define GPU size parameters\n",
    "        self.cores = cores\n",
    "        if self.cores % self.N is not 0:\n",
    "            raise ValueError('The number of cores is not divisible by '\n",
    "                             + 'the dimension N.')\n",
    "        self.threadsX = int(self.cores/self.N)\n",
    "        if self.M % self.threadsX is not 0:\n",
    "            raise ValueError('The number of observations is not divisible '\n",
    "                               + 'by the number of threads per block.')\n",
    "        self.NUMBER_OF_BLOCKS = int(self.M*self.N/self.cores)\n",
    "        self.THREADS_X_PER_BLOCK = self.threadsX\n",
    "        self.THREADS_Y_PER_BLOCK = self.N\n",
    "\n",
    "        # Define a string object containing the kernel\n",
    "        kernel_code = \"\"\"\n",
    "            #include <stdio.h>\n",
    "            __global__ void L2_norm(float *a, float *b)\n",
    "            {\n",
    "                // 2D Thread ID to calculate L2 norm\n",
    "                // index to entry in a\n",
    "                const int ida = threadIdx.y + threadIdx.x*blockDim.y + blockIdx.x*blockDim.y*blockDim.x;\n",
    "                // Index to entry in b\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                int k = blockDim.y / 2; //the number of columns in a block to pairwise sum\n",
    "\n",
    "                // Square the entries in the sum\n",
    "                a[ida] *= a[ida];\n",
    "                if (511 == ida)\n",
    "                {\n",
    "                    printf(\"%d, %d, %d, %d \\\\n\", blockDim.y, blockDim.x, ida, idb);\n",
    "                }\n",
    "                // Carry out pairwise summation of squared entries along row\n",
    "                // to calculate the L2 norm\n",
    "                while (k != 0)\n",
    "                {\n",
    "                    if (threadIdx.y < k)\n",
    "                        a[ida] += a[ida + k];\n",
    "                    __syncthreads();\n",
    "                    k /= 2;\n",
    "                }\n",
    "\n",
    "                // Output to b\n",
    "                if (0 == threadIdx.y)\n",
    "                {\n",
    "                    b[idb] = a[ida];\n",
    "                }\n",
    "            }\n",
    "\n",
    "            __global__ void sum1(float *a, float *b, float *c)\n",
    "            {\n",
    "                // Object to store pairwise sum\n",
    "                __shared__ float temp2[512];\n",
    "\n",
    "                // Index to entry\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                // Constants for pairwise sum\n",
    "                int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "\n",
    "                // Carry out the elementwise multiplication\n",
    "                temp2[threadIdx.x] = a[idb]*b[idb];\n",
    "\n",
    "                __syncthreads();\n",
    "\n",
    "                //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "                while (n != 0)\n",
    "                {\n",
    "                    if (threadIdx.x < n)\n",
    "                        temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "                    __syncthreads();\n",
    "                    n /= 2;\n",
    "                }\n",
    "                //add the total to the sum only once per block\n",
    "                if (0 == threadIdx.x)\n",
    "                    atomicAdd(c,temp2[0]);\n",
    "            }\n",
    "\n",
    "\n",
    "            __global__ void var1(float *a, float *b, float *c, float *d)\n",
    "            {\n",
    "                // Objects to store pairwise sums\n",
    "                __shared__ float temp2[512];\n",
    "\n",
    "                // Index to entry in b\n",
    "                const int idb = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "\n",
    "                // Constants for pairwise sum\n",
    "                int n = blockDim.x / 2; //the number of rows in a block to pairwise sum\n",
    "\n",
    "                // Carry out the elementwise multiplication\n",
    "                // NOTE: only on left most row entry\n",
    "                //printf(\"%f \\\\n\", d[0]);\n",
    "                if (0 == threadIdx.y)\n",
    "                    temp2[threadIdx.x] = a[idb]*b[idb];\n",
    "                    temp2[threadIdx.x] -= d[0];\n",
    "                    temp2[threadIdx.x] *= temp2[threadIdx.x];\n",
    "\n",
    "                __syncthreads();\n",
    "\n",
    "                //now pairwise sum the vector temp2, store in temp2, then output to c\n",
    "                while (n != 0)\n",
    "                {\n",
    "                    if (threadIdx.x < n)\n",
    "                        temp2[threadIdx.x] += temp2[threadIdx.x+n];\n",
    "                    __syncthreads();\n",
    "                    n /= 2;\n",
    "                }\n",
    "\n",
    "                //add the total to the sum only once per block\n",
    "                if (0 == threadIdx.x)\n",
    "                    atomicAdd(c,temp2[0]);\n",
    "            }\n",
    "\n",
    "            \"\"\"\n",
    "        # Pass to PyCUDA the size of the matrix\n",
    "        #kernel_code = kernel_code % {\n",
    "        #    'MATRIX_SIZE': self.THREADS_X_PER_BLOCK,\n",
    "        #    }\n",
    "\n",
    "        # compile the kernel code\n",
    "        mod = SourceModule(kernel_code)\n",
    "\n",
    "        # Define a function for the parallel sum\n",
    "        self.gpu_sum1 = mod.get_function(\"sum1\")\n",
    "        self.gpu_var1 = mod.get_function(\"var1\")\n",
    "        self.gpu_L2 = mod.get_function(\"L2_norm\")\n",
    "\n",
    "    def estimate(self):\n",
    "        \"\"\"\n",
    "        A GPU implementation of monte carlo mean.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the probability of the observations in serial\n",
    "        # NOTE: Even more speed up available here by parallelization\n",
    "        self.g_of_points = self.g(self.points, self.X).astype(np.float32)\n",
    "\n",
    "        # Allocate memory to fill with solution\n",
    "        sum1 = np.zeros(1).astype(np.float32)\n",
    "\n",
    "        # Allocate memory on the gpu\n",
    "        a_gpu = cuda.mem_alloc(self.points.astype(np.float32).nbytes)\n",
    "        a_temp_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        b_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        c_gpu = cuda.mem_alloc(sum1.nbytes)\n",
    "\n",
    "        # Transfer data to the device\n",
    "        cuda.memcpy_htod(a_gpu, self.points.astype(np.float32))\n",
    "        cuda.memcpy_htod(a_temp_gpu, np.zeros(self.M).astype(np.float32))\n",
    "        cuda.memcpy_htod(b_gpu, self.g_of_points)\n",
    "        cuda.memcpy_htod(c_gpu, sum1)\n",
    "\n",
    "        # Carry out the multiplication\n",
    "        self.gpu_L2(a_gpu, a_temp_gpu, block=(self.THREADS_X_PER_BLOCK,\n",
    "                                              self.THREADS_Y_PER_BLOCK, 1),\n",
    "                    grid=(self.NUMBER_OF_BLOCKS, 1))\n",
    "        self.gpu_sum1(a_temp_gpu, b_gpu, c_gpu,\n",
    "                      block=(self.cores, 1, 1),\n",
    "                      grid=(int(self.NUMBER_OF_BLOCKS/self.N), 1))\n",
    "\n",
    "        # Retrieve data from host\n",
    "        cuda.memcpy_dtoh(sum1, c_gpu)\n",
    "\n",
    "        # Calculate the mean\n",
    "        self.mean = sum1*self.V/self.M\n",
    "\n",
    "        # Clean up memory\n",
    "        del a_gpu, c_gpu\n",
    "\n",
    "        # Calculate the probability of the observations in serial\n",
    "        # NOTE: Even more speed up available here by parallelization\n",
    "        # NOTE: I saved this information from previous calculation\n",
    "        #g_of_points = self.g(self.points, self.X)\n",
    "        # Calculate parameter\n",
    "        d = np.array((self.mean/self.V), dtype=np.float32)\n",
    "\n",
    "        # Allocate memory to fill with solution\n",
    "        var1 = np.zeros(1).astype(np.float32)\n",
    "\n",
    "        # Allocate memory on the gpu\n",
    "        #a_gpu = cuda.mem_alloc(self.points.astype(np.float32).nbytes)\n",
    "        #a_temp_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        b_gpu = cuda.mem_alloc(self.g_of_points.astype(np.float32).nbytes)\n",
    "        c_gpu = cuda.mem_alloc(var1.nbytes)\n",
    "        d_gpu = cuda.mem_alloc(d.nbytes)\n",
    "\n",
    "        # Transfer data to the device\n",
    "        #cuda.memcpy_htod(a_gpu, self.points.astype(np.float32))\n",
    "        cuda.memcpy_htod(b_gpu, self.g_of_points.astype(np.float32))\n",
    "        cuda.memcpy_htod(c_gpu, var1)\n",
    "        cuda.memcpy_htod(d_gpu, d)\n",
    "\n",
    "        # Carry out the multiplication\n",
    "        #self.gpu_L2(a_gpu, a_temp_gpu, block=(self.THREADS_X_PER_BLOCK,\n",
    "        #                                      self.THREADS_Y_PER_BLOCK, 1),\n",
    "        #            grid=(self.NUMBER_OF_BLOCKS, 1))\n",
    "        self.gpu_var1(a_temp_gpu, b_gpu, c_gpu, d_gpu,\n",
    "                      block=(self.cores, 1, 1),\n",
    "                      grid=(int(self.NUMBER_OF_BLOCKS/self.N), 1))\n",
    "\n",
    "        # Retrieve data from host\n",
    "        cuda.memcpy_dtoh(var1, c_gpu)\n",
    "\n",
    "        # Calculate the variance\n",
    "        self.var = var1*self.V**2/(self.M*(self.M - 1))\n",
    "\n",
    "        # Clean up memory\n",
    "        del b_gpu, c_gpu, d_gpu\n",
    "        return self.mean, self.var\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        A method to calculate monte carlo integral and variance of estimate.\n",
    "        \"\"\"\n",
    "        return (self.estimate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "M = 512\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.045313358306884766 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0010352134704589844 seconds \n",
      "\n",
      "2.65155342787 [ 2.84231591]\n",
      "0.00143525759458 [ 0.00144499]\n",
      "\n",
      "\n",
      "M = 5120\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.052376508712768555 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0014107227325439453 seconds \n",
      "\n",
      "2.67685436261 [ 2.83520079]\n",
      "0.000139364237776 [ 0.00014605]\n",
      "\n",
      "\n",
      "M = 51200\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.05659008026123047 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.005671262741088867 seconds \n",
      "\n",
      "2.67080567941 [ 2.87825203]\n",
      "1.39140496787e-05 [  1.47154911e-05]\n",
      "\n",
      "\n",
      "M = 512000\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.10376501083374023 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.040251731872558594 seconds \n",
      "\n",
      "2.66728599415 [ 2.85387769]\n",
      "1.38747953048e-06 [  1.45752321e-06]\n",
      "\n",
      "\n",
      "M = 5120000\n",
      "CPU PARALLEL NUMPY\n",
      "Executed in 0.6914644241333008 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.3931279182434082 seconds \n",
      "\n",
      "2.66672165535 [ 2.8569416]\n",
      "1.38968790511e-07 [  1.45831385e-07]\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "N = 8\n",
    "cores = 512\n",
    "threadsX = int(cores/N)\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "# NOTE: For optimization reasons, M >= N => #observations/#dimensions >=1\n",
    "for M in [N, N*10, N*100, N*1000, N*10000]:\n",
    "    M *= threadsX\n",
    "    print(\"\\n\\nM = %s\" % M)\n",
    "    test_numpy = cpu_parallel_monte_carlo_numpy(f_vec, g_vec, M, X, workers)\n",
    "    test_gpu = GPU_parallel_L2_monte_carlo_block(g_vec, M, X, cores)\n",
    "    #print(\"SERIAL NATIVE\")\n",
    "    #%timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "    #print(\"SERIAL NUMPY\")\n",
    "    #%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "    print(\"CPU PARALLEL NUMPY\")\n",
    "    start = time.time()\n",
    "    test_numpy.output()\n",
    "    print(\"Executed in %s seconds \\n\" % (time.time() - start))\n",
    "    print(\"GPU FULLY PARALLEL\")\n",
    "    start = time.time()\n",
    "    test_gpu.output()\n",
    "    print(\"Executed in %s seconds \\n\" % (time.time() - start))\n",
    "    print(test_numpy.mean, test_gpu.mean)\n",
    "    print(test_numpy.var, test_gpu.var)\n",
    "    del test_numpy\n",
    "    del test_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, calculating the L2 norm slows us down.  We get an order of magnitude faster for lower values of M and double the speed for M = 5120000.\n",
    "\n",
    "An issue here is that the solution is diverging rather than converging.  There must be a mistake somewhere in the code, but alas, I'm lost as to where it is.\n",
    "\n",
    "A final question to ask is whether the GPU solution beats NumPy.  Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "M = 512\n",
      "PURE NUMPY\n",
      "Executed in 0.0007174015045166016 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0007171630859375 seconds \n",
      "\n",
      "2.72271814003 [ 2.93542123]\n",
      "0.00129062974425 [ 0.00135468]\n",
      "\n",
      "\n",
      "M = 5120\n",
      "PURE NUMPY\n",
      "Executed in 0.00506591796875 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.0011417865753173828 seconds \n",
      "\n",
      "2.66298405365 [ 2.8684392]\n",
      "0.000140959950759 [ 0.00014492]\n",
      "\n",
      "\n",
      "M = 51200\n",
      "PURE NUMPY\n",
      "Executed in 0.05710792541503906 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.005090236663818359 seconds \n",
      "\n",
      "2.66582038948 [ 2.87064481]\n",
      "1.38518394922e-05 [  1.46132507e-05]\n",
      "\n",
      "\n",
      "M = 512000\n",
      "PURE NUMPY\n",
      "Executed in 0.5198211669921875 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.04703259468078613 seconds \n",
      "\n",
      "2.66729774271 [ 2.85724609]\n",
      "1.38971480731e-06 [  1.45909749e-06]\n",
      "\n",
      "\n",
      "M = 5120000\n",
      "PURE NUMPY\n",
      "Executed in 5.03239107131958 seconds \n",
      "\n",
      "GPU FULLY PARALLEL\n",
      "Executed in 0.39262962341308594 seconds \n",
      "\n",
      "2.66706278055 [ 2.8562916]\n",
      "1.38787050164e-07 [  1.45985689e-07]\n"
     ]
    }
   ],
   "source": [
    "N = 8\n",
    "cores = 512\n",
    "threadsX = int(cores/N)\n",
    "X = np.array((0*np.ones(N), 1*np.ones(N))).T\n",
    "\n",
    "# NOTE: For optimization reasons, M >= N => #observations/#dimensions >=1\n",
    "for M in [N, N*10, N*100, N*1000, N*10000]:\n",
    "    M *= threadsX\n",
    "    print(\"\\n\\nM = %s\" % M)\n",
    "    test_gpu = GPU_parallel_L2_monte_carlo_block(g_vec, M, X, cores)\n",
    "    #print(\"SERIAL NATIVE\")\n",
    "    #%timeit monte_carlo_expectation_serial(f, g, M, X)\n",
    "    #print(\"SERIAL NUMPY\")\n",
    "    #%timeit monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "    print(\"PURE NUMPY\")\n",
    "    start = time.time()\n",
    "    test_numpy = monte_carlo_expectation_numpy(f_vec, g_vec, M, X)\n",
    "    print(\"Executed in %s seconds \\n\" % (time.time() - start))\n",
    "    print(\"GPU FULLY PARALLEL\")\n",
    "    start = time.time()\n",
    "    test_gpu.output()\n",
    "    print(\"Executed in %s seconds \\n\" % (time.time() - start))\n",
    "    print(test_numpy[0], test_gpu.mean)\n",
    "    print(test_numpy[1], test_gpu.var)\n",
    "    del test_numpy\n",
    "    del test_gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes!  For high values of N (ie >> 2) we do indeed get speed.\n",
    "\n",
    "### Conclusion\n",
    "This class saw applications of Python, NumPy, Numba, multiprocessing, and GPU multithreading for increasing speed.  Hopefully you should be familiar with how these work and the complexity of the approach for each.  I hope you enjoyed the class and that in the future you consider Python as a part of your own toolbox for numerical problem solving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
